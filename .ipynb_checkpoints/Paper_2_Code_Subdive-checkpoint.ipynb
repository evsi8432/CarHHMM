{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp\n",
    "from scipy.optimize import LinearConstraint\n",
    "\n",
    "import divebomb\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crude_timescale = 10 #(in minutes)\n",
    "\n",
    "N = 3 # number of possible production states S_i\n",
    "dims = ['td_total_duration','max_depth','bottom_variance']\n",
    "d = len(dims) # dimension of Y_t\n",
    "\n",
    "K = 2 # number of internal states H_i \n",
    "\n",
    "diff_norms = False # Should each crude state have its own emission distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_depth = '../Data/20190902-182840-CATs_OB_1_001_JustDepth_Processed.csv'\n",
    "just_depth_cols = ['time','hr_from_start','depth']\n",
    "\n",
    "all_data = '../Data/20190902-182840-CATs_OB_1_001_Plot_Processed.csv'\n",
    "all_cols = ['time','hr_from_start','depth','Ax','Ay','Az','Gx','Gy','Gz','Mx','My','Mz']\n",
    "\n",
    "df = pd.read_csv(all_data)\n",
    "\n",
    "df['depth'] = df['Depth_m']\n",
    "df = df[all_cols]\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "df = df[df['time'] >= '2019-09-02 13:20:00']\n",
    "df = df[df['time'] <= '2019-09-02 18:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.tail())\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Elevation'] = -df['depth']\n",
    "df.plot(style='.',x='hr_from_start',y='Elevation',figsize=(40,5),markersize = '1')\n",
    "plt.axvline(1.6,color='k')\n",
    "plt.axvline(1.8,color='k')\n",
    "plt.axvline(3.3,color='k')\n",
    "plt.axvline(4.3,color='k')\n",
    "plt.savefig('../Plots/raw_data.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df[(df['hr_from_start'] < 1.6) | (df['hr_from_start'] > 1.8)]\n",
    "df = df[(df['hr_from_start'] < 3.3) | (df['hr_from_start'] > 4.3)]\n",
    "\n",
    "def get_time_series(x):\n",
    "    if x < 1.6:\n",
    "        return 0\n",
    "    elif x < 3.3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "df['time_series'] = df['hr_from_start'].apply(get_time_series)\n",
    "\n",
    "\n",
    "ax = df[df['time_series'] == 0].plot(style='.',\n",
    "                                     x='hr_from_start',\n",
    "                                     y='depth',\n",
    "                                     figsize=(40,5),\n",
    "                                     markersize = '1')\n",
    "df[df['time_series'] == 1].plot(style='.',\n",
    "                                x='hr_from_start',\n",
    "                                y='depth',\n",
    "                                figsize=(40,5),\n",
    "                                markersize = '1',\n",
    "                                ax=ax)\n",
    "df[df['time_series'] == 2].plot(style='.',\n",
    "                                x='hr_from_start',\n",
    "                                y='depth',\n",
    "                                figsize=(40,5),\n",
    "                                markersize = '1',\n",
    "                                ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['time'] == '2019-09-02 13:20:00.000']))\n",
    "print(len(df[df['time'] == '2019-09-02 14:20:00.000']))\n",
    "print(len(df[df['time'] == '2019-09-02 15:20:00.000']))\n",
    "print(len(df[df['time'] == '2019-09-02 16:20:00.000']))\n",
    "print(len(df[df['time'] == '2019-09-02 17:20:00.000']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dive_display = divebomb.profile_dives(df,\n",
    "                                      minimal_time_between_dives=10, \n",
    "                                      surface_threshold=3.1,\n",
    "                                      ipython_display_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = divebomb.profile_dives(df,\n",
    "                                  minimal_time_between_dives=10, \n",
    "                                  surface_threshold=3.1,\n",
    "                                  ipython_display_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dives = raw_data[0]\n",
    "dives = pd.merge(dives,\n",
    "                 raw_data[2][['time','time_series']].rename(columns={'time':'bottom_start'}).drop_duplicates(),\n",
    "                 on='bottom_start')\n",
    "\n",
    "dives = dives.drop(len(dives)-1)\n",
    "dives = dives.drop(0)\n",
    "dives = dives[dives['td_total_duration'] < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_temp = raw_data[2]\n",
    "df_temp = df_temp.reset_index().drop(columns=['index'])\n",
    "dive_nums = pd.Series([-1]*len(df))\n",
    "for dive_num,(s,e) in enumerate(zip(dives['dive_start'],dives['dive_end'])):\n",
    "    ind = (df_temp['time'] > s) & (df_temp['time'] < e)\n",
    "    dive_nums[ind] = dive_num\n",
    "    \n",
    "df_temp['dive_num'] = dive_nums\n",
    "df_temp['time'] = list(df['time'])\n",
    "df = df_temp.copy()\n",
    "del df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dive in range(5):#max(df['dive_num'])):\n",
    "    if np.nanmax(df[df['dive_num'] == dive]['Ax'].diff()) > 10:\n",
    "        print(dive)\n",
    "        df[df['dive_num'] == dive]['Ax'].diff().plot()\n",
    "        plt.show()\n",
    "        df[df['dive_num'] == dive].plot(x='hr_from_start',y='Ax')\n",
    "        plt.show()\n",
    "        df[df['dive_num'] == dive].plot(x='hr_from_start',y='Ay')\n",
    "        plt.show()\n",
    "        df[df['dive_num'] == dive].plot(x='hr_from_start',y='Az')\n",
    "        plt.show()\n",
    "        df[df['dive_num'] == dive].plot(x='hr_from_start',y='elevation')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dive_num in range(5):\n",
    "    plt.plot(df[df['dive_num'] == dive_num]['Ax'],df[df['dive_num'] == dive_num]['Ay'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dives[['td_total_duration','max_depth','bottom_variance']].hist(figsize = (10,10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Likelihood and Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_p(Y,eta,theta,drop_dive=(-1,-1),cdf_dim=-1):\n",
    "    \n",
    "    # Find number of states\n",
    "    N = eta.shape[0]\n",
    "    \n",
    "    # find the tpm\n",
    "    np.fill_diagonal(eta, 0)\n",
    "    ptm = np.exp(eta)\n",
    "    ptm = (ptm.T/np.sum(ptm,1)).T\n",
    "    \n",
    "    # find the covariance matrix by getting gram matrix\n",
    "    mu = theta[:,:,0]\n",
    "    Sigma = np.array([np.matmul(th[:,1:].T,th[:,1:]) for th in theta])\n",
    "    \n",
    "    # find the initial distribution (stationary distribution)\n",
    "    delta = np.ones((1,N))/N\n",
    "    for _ in range(10):\n",
    "        delta = delta.dot(ptm)\n",
    "    \n",
    "    # overall likelihood\n",
    "    L_p = 0\n",
    "    \n",
    "    # iterate through all 3 time series\n",
    "    for i,Y0 in enumerate(Y):\n",
    "        \n",
    "        # initialize values\n",
    "        phi = delta\n",
    "        L_p0 = 0\n",
    "\n",
    "        # now find the likelihood of observations, adjusting for vanishing gradients \n",
    "        for j,y_t in enumerate(Y0):\n",
    "            \n",
    "            # skip dives to use for psuedoresiduals\n",
    "            if i == drop_dive[0] and j == drop_dive[1]:\n",
    "                if cdf_dim != -1:\n",
    "                    \n",
    "                    F_yt_given_xt = np.zeros(N)\n",
    "                    \n",
    "                    for n in range(N):\n",
    "                        Sigma11 = Sigma[n,cdf_dim,cdf_dim]\n",
    "                        Sigma12 = np.delete(Sigma[n,cdf_dim:cdf_dim+1,:],cdf_dim,1)\n",
    "                        Sigma21 = np.delete(Sigma[n,:,cdf_dim:cdf_dim+1],cdf_dim,0)\n",
    "                        Sigma22 = np.delete(np.delete(Sigma[n,:,:],cdf_dim,0),cdf_dim,1)\n",
    "                        Sigma22_inv = np.linalg.inv(Sigma22)\n",
    "\n",
    "                        mu1 = mu[n,cdf_dim]\n",
    "                        mu2 = np.delete(mu[n,:],cdf_dim)\n",
    "\n",
    "                        y1 = y_t[cdf_dim]\n",
    "                        y2 = np.delete(y_t,cdf_dim)\n",
    "\n",
    "                        #print(mu1)\n",
    "                        #print(Sigma12)\n",
    "                        #print(Sigma22_inv)\n",
    "                        #print(y2)\n",
    "                        #print(mu2)\n",
    "                        \n",
    "                        mu_bar = mu1 + Sigma12.dot(Sigma22_inv).dot(y2-mu2)\n",
    "                        Sigma_bar = Sigma11 - Sigma12.dot(Sigma22_inv).dot(Sigma21)\n",
    "\n",
    "                        F_yt_given_xt[n] = multivariate_normal.cdf(y_t[cdf_dim],mu_bar,Sigma_bar)\n",
    "                        \n",
    "                    v = phi.dot(ptm)*F_yt_given_xt\n",
    "\n",
    "                else:\n",
    "                    v = phi.dot(ptm)\n",
    "\n",
    "            else:\n",
    "                \n",
    "                p_yt_given_xt = np.zeros(N)\n",
    "                for n in range(N):\n",
    "                    p_yt_given_xt[n] = multivariate_normal.pdf(y_t,mu[n],cov=Sigma[n],allow_singular=True)\n",
    "                    \n",
    "                v = phi.dot(ptm)*p_yt_given_xt\n",
    "                \n",
    "            u = np.sum(v)\n",
    "            L_p0 += np.log(u)\n",
    "            phi = v/u\n",
    "            \n",
    "        L_p += L_p0\n",
    "\n",
    "    return L_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get relevant info from Y\n",
    "Y = []\n",
    "gbo = dives.groupby('time_series')\n",
    "for key in dives.groupby('time_series').groups:\n",
    "    Y.append(gbo.get_group(key)[dims].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial guesses for theta\n",
    "N = 3\n",
    "d = 3\n",
    "\n",
    "mu0 = 0.1*np.random.normal(size=(N,d))\n",
    "sig0 = 0.001*(np.random.normal(size=(N,d,d)))**2\n",
    "theta = np.zeros(shape=(N,d,d+1))\n",
    "\n",
    "mu0[:,0] += [10,75,200] # means of dive duration\n",
    "sig0[:,0,0] += [5,20,40] # std of dive duration\n",
    "\n",
    "mu0[:,1] += [5,10,20] # mean of dive depth\n",
    "sig0[:,1,1] += [2,4,10] # std of dive depth\n",
    "\n",
    "mu0[:,2] += [0.2,0.6,1.2] # mean of dive wiggliness\n",
    "sig0[:,2,2] += [0.1,0.1,0.1] # std of dive wiggliness\n",
    "\n",
    "theta[:,:,0] = mu0\n",
    "theta[:,:,1:] = sig0\n",
    "\n",
    "theta = np.maximum(theta,10e-6)\n",
    "\n",
    "# randomly initialize the ptm\n",
    "eta = -1.0 + np.random.normal(size=(N,N))\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    l = likelihood_p(Y,eta,theta)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x):\n",
    "    eta = x[:N**2].reshape((N,N))\n",
    "    theta = np.reshape(x[N**2:],(N,d,d+1))\n",
    "    return -likelihood_p(Y,eta,theta)\n",
    "\n",
    "# note there are better and faster ways to do this, notably with the baum-welch algorithm, but\n",
    "# 5 minutes is okay with me, and the results look alright\n",
    "\n",
    "# try to load what has been saved\n",
    "try:\n",
    "    with open('../Data/backup_divebomb_norm/naive_params_%d.pkl' % N, 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "        \n",
    "# if nothing has been saved, find parameters again\n",
    "except FileNotFoundError:\n",
    "    x0 = np.concatenate([eta.ravel(),theta.ravel()])\n",
    "    start = time.time()\n",
    "    res = minimize(loss_fn, x0, method = 'Nelder-Mead', options={'disp': True, 'adaptive':True})\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    \n",
    "    with open('../Data/backup_divebomb_norm/naive_params_%d.pkl' % N, 'wb') as f: \n",
    "        pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = res['x'][:N**2].reshape((N,N))\n",
    "theta = np.reshape(res['x'][N**2:],(N,d,d+1))\n",
    "\n",
    "eta = np.array(eta)\n",
    "theta = np.array(theta)\n",
    "\n",
    "np.fill_diagonal(eta, 0)\n",
    "\n",
    "ptm = np.exp(eta)\n",
    "ptm = (ptm.T/np.sum(ptm,1)).T\n",
    "\n",
    "delta = np.ones((1,N))/N\n",
    "for _ in range(10):\n",
    "    delta = delta.dot(ptm)\n",
    "\n",
    "print('Probability Transition Matrix:')\n",
    "print(ptm)\n",
    "print('')\n",
    "print('Stationary Distribution:')\n",
    "print(delta)\n",
    "print('')\n",
    "print('Means:')\n",
    "print(theta[:,:,0])\n",
    "print('')\n",
    "print('Covaraince Matrices:')\n",
    "for n in range(N):\n",
    "    print(theta[n,:,1:].T.dot(theta[n,:,1:]))\n",
    "    print('')\n",
    "\n",
    "titles = ['Dive Duration (seconds)','Maximum Depth (meters)','Bottom Variance (meters^2)']\n",
    "xlabels = ['Seconds','Meters','Meters^2']\n",
    "xlims = [[0,300],[0,50],[0,3]]\n",
    "\n",
    "for i,state_th in enumerate(theta):\n",
    "    \n",
    "    for j,dim_th in enumerate(state_th):\n",
    "        \n",
    "        plt.figure(j+1)\n",
    "        x = np.linspace(xlims[j][0],xlims[j][1],10000)\n",
    "        \n",
    "        sig2 = state_th[:,1:].T.dot(state_th[:,1:])\n",
    "        y = norm.pdf(x,dim_th[0],np.sqrt(sig2[j,j]))\n",
    "        plt.plot(x,y)\n",
    "        \n",
    "        plt.title(titles[j])\n",
    "        plt.xlabel(xlabels[j])\n",
    "        plt.ylabel('Probability Density')\n",
    "        plt.legend(['Production State %d' % k for k in range(d)])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the Virterbi Algorithm to find most likely state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from stack-overflow\n",
    "\n",
    "def viterbi(Y, ptm, theta):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the MAP estimate of state trajectory of Hidden Markov Model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : array (T,d)\n",
    "        Observation state sequence. int dtype.\n",
    "    ptm : array (N, N)\n",
    "        State transition matrix.\n",
    "    theta : array (N, d, d+1)\n",
    "        Emission parameters for normal distribution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : array (T,)\n",
    "        Maximum a posteriori probability estimate of hidden state trajectory,\n",
    "        conditioned on observation sequence y under the model parameters A, B,\n",
    "        Pi.\n",
    "    T1: array (K, T)\n",
    "        the probability of the most likely path so far\n",
    "    T2: array (K, T)\n",
    "        the x_j-1 of the most likely path so far\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cardinality of the state space\n",
    "    N = ptm.shape[0]\n",
    "    \n",
    "    # get approx stationary dist.\n",
    "    delta = np.ones((1,N))/N\n",
    "    for _ in range(10):\n",
    "        delta = delta.dot(ptm)\n",
    "    \n",
    "    # get mu and Sigma\n",
    "    mu = theta[:,:,0]\n",
    "    Sigma = np.array([np.matmul(th[:,1:].T,th[:,1:]) for th in theta])\n",
    "    \n",
    "    T = Y.shape[0]\n",
    "    T1 = np.zeros((T,N)) # probs of most likely\n",
    "    T2 = np.zeros((T,N)) # most likely state\n",
    "    \n",
    "    # Initilaize the tracking tables from first observation\n",
    "    log_p_y0_given_s0 = np.zeros(N)\n",
    "    for n in range(N):\n",
    "        log_p_y0_given_s0[n] = multivariate_normal.logpdf(Y[0],mu[n],cov=Sigma[n],allow_singular=True)\n",
    "        \n",
    "    T1[0,:] = np.log(delta) + log_p_y0_given_s0\n",
    "    T2[0,:] = -1\n",
    "    \n",
    "    phi = delta\n",
    "    L_p = 0\n",
    "\n",
    "    # Iterate throught the observations updating the tracking tables\n",
    "    for t in range(1,T):\n",
    "        \n",
    "        log_p_yt_given_st = np.zeros(N)\n",
    "        for n in range(N):\n",
    "            log_p_yt_given_st[n] = multivariate_normal.logpdf(Y[t],mu[n],cov=Sigma[n],allow_singular=True)\n",
    "        \n",
    "        for n in range(N):\n",
    "            \n",
    "            T1[t,n] = np.max(T1[t-1,:] + np.log(ptm[:,n]) + log_p_yt_given_st[n])\n",
    "            T2[t,n] = np.argmax(T1[t-1,:] + np.log(ptm[:,n]))\n",
    "\n",
    "    # Build the output, optimal model trajectory\n",
    "    x = -1 * np.ones(T)\n",
    "    x[-1] = np.argmax(T1[T-1,:])\n",
    "    for t in reversed(range(1, T)):\n",
    "        x[t-1] = T2[t, int(x[t])]\n",
    "\n",
    "    return x, T1, T2\n",
    "\n",
    "s_hat = []\n",
    "T1 = []\n",
    "T2 = []\n",
    "\n",
    "for Y0 in Y:\n",
    "    s_hat0,T10,T20 = viterbi(Y0,ptm,theta)\n",
    "    \n",
    "    s_hat.append(s_hat0)\n",
    "    T1.append(T10)\n",
    "    T2.append(T20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the state over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dives['predicted_production_state'] = np.concatenate(s_hat)\n",
    "\n",
    "plt.subplots(N,d,figsize=(20,20))\n",
    "xlims = [[0,300],[0,50],[0,3]]\n",
    "\n",
    "for i,(state_th,s) in enumerate(zip(theta,[0.0,1.0,2.0])):\n",
    "    for j,(dim_th,col) in enumerate(zip(state_th,dims)):\n",
    "        \n",
    "        plt.subplot(N,d,i*d + j + 1)\n",
    "        \n",
    "        dives[dives['predicted_production_state'] == s][col].hist(density = True)\n",
    "        \n",
    "        x = np.linspace(xlims[j][0],xlims[j][1],10000)\n",
    "        sig2 = state_th[:,1:].T.dot(state_th[:,1:])\n",
    "        y = norm.pdf(x,dim_th[0],np.sqrt(sig2[j,j]))\n",
    "        plt.plot(x,y)\n",
    "        \n",
    "        plt.title('%s, Production State %d' % (col,int(s)))\n",
    "        plt.xlabel(xlabels[j])\n",
    "        plt.ylabel('Probability Density')\n",
    "        plt.legend(['Predicted Density','Observed Density'])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = dives['dive_start'].iloc[0]\n",
    "dives['dive_start'] = dives['dive_start']-start_time\n",
    "dives['dive_end'] = dives['dive_end']-start_time\n",
    "\n",
    "Y_concat = np.concatenate(Y)\n",
    "\n",
    "plt.subplots(4,1,figsize=(10,20))\n",
    "ylabels = ['Dive Duration (seconds)',\n",
    "           'Maximum Depth (meters)',\n",
    "           'Bottom Variance (meters^2)']\n",
    "\n",
    "for i in range(d):\n",
    "    plt.subplot(4,1,i+1)\n",
    "    plt.scatter(dives['dive_start'],Y_concat[:,i],c=dives['predicted_production_state'],cmap='rainbow',s=60)\n",
    "    plt.plot(dives['dive_start'],Y_concat[:,i],'k--')\n",
    "    plt.ylabel(ylabels[i])\n",
    "    \n",
    "plt.subplot(4,1,4)\n",
    "plt.plot(dives['dive_start'],dives['predicted_production_state'],'.')\n",
    "plt.ylabel('Predicted Production State')\n",
    "plt.xlabel('Second After Start')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce Hierachial Strucutre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide up Y into crude time indices\n",
    "dives['crude_time_index'] = dives['dive_start'].apply(lambda x: int(x/(crude_timescale*60)))\n",
    "\n",
    "Y = [[] for _ in range(dives['time_series'].nunique())]\n",
    "gbo = dives.groupby(['time_series','crude_time_index'])\n",
    "\n",
    "for key,group in gbo:\n",
    "    Y[key[0]].append(group[dims].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_h(Y,eta_crude,eta_fine,theta_fine,diff_norms=False,drop_dive=(-1,-1,-1),cdf_dim=-1):\n",
    "    \n",
    "    # Find number of crude states\n",
    "    K = eta_crude.shape[0]\n",
    "    \n",
    "    # find the tpm\n",
    "    np.fill_diagonal(eta_crude, 0)\n",
    "    ptm = np.exp(eta_crude)\n",
    "    ptm = (ptm.T/np.sum(ptm,1)).T\n",
    "    \n",
    "    # find the initial distribution (stationary distribution)\n",
    "    delta = np.ones((1,K))/K\n",
    "    for _ in range(10):\n",
    "        delta = delta.dot(ptm)\n",
    "    \n",
    "    # initialize hierarchical likelihood\n",
    "    L_h = 0\n",
    "    \n",
    "    # for every time series, do this\n",
    "    for i,Y0 in enumerate(Y):\n",
    "        \n",
    "        # initialize values\n",
    "        phi = delta\n",
    "        L_h0 = 0\n",
    "\n",
    "        # now find the likelihood of observations, adjusting for vanishing gradients \n",
    "        for j,Yk in enumerate(Y0):\n",
    "            \n",
    "            log_p_Yk_given_st = np.zeros(K)\n",
    "            \n",
    "            for k,(eta,theta) in enumerate(zip(eta_fine,theta_fine)):\n",
    "                \n",
    "                if i == drop_dive[0] and j == drop_dive[1]:\n",
    "                    log_p_Yk_given_st[k] = likelihood_p([Yk],eta,theta,\n",
    "                                                        drop_dive=(0,drop_dive[2]),cdf_dim=cdf_dim)\n",
    "                else:\n",
    "                    log_p_Yk_given_st[k] = likelihood_p([Yk],eta,theta)\n",
    "                    \n",
    "            log_v = np.log(phi.dot(ptm)) + log_p_Yk_given_st\n",
    "            log_u = logsumexp(log_v)\n",
    "            L_h0 += log_u\n",
    "            phi = np.exp(log_v-log_u)\n",
    "            \n",
    "        L_h += L_h0\n",
    "\n",
    "    return L_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eta_crude = np.random.normal(size=(K,K))\n",
    "\n",
    "eta_fine = np.array([eta]*K)\n",
    "eta_fine = eta_fine + 0.1*np.random.normal(size=eta_fine.shape)\n",
    "\n",
    "theta_fine = np.array(theta)\n",
    "\n",
    "for n in range(N):\n",
    "    C = dives[dives['predicted_production_state'] == float(s)][dims].cov().to_numpy()\n",
    "    eigs,vs = np.linalg.eig(C)\n",
    "    A = np.diag(np.sqrt(eigs)).dot(vs)\n",
    "    theta_fine[n,:,1:] = A\n",
    "    \n",
    "if diff_norms:\n",
    "    theta_fine = np.array([theta_fine]*K)\n",
    "    theta_fine = theta_fine + 0.001*np.random.normal(size=theta_fine.shape)\n",
    "    theta_fine = np.maximum(theta_fine,10e-8)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    if diff_norms:\n",
    "        l = likelihood_h(Y,eta_crude,eta_fine,theta_fine)\n",
    "    else:\n",
    "        l = likelihood_h(Y,eta_crude,eta_fine,np.array([theta_fine]*K))\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x):\n",
    "    eta_crude = x[:K**2].reshape((K,K))\n",
    "    eta_fine = x[K**2:K**2+K*(N**2)].reshape(K,N,N)\n",
    "    if diff_norms:\n",
    "        theta_fine = np.reshape(x[K**2+K*(N**2):],(K,N,d,d+1))\n",
    "    else:\n",
    "        theta_fine = np.reshape(x[K**2+K*(N**2):],(N,d,d+1))\n",
    "        theta_fine = np.array([theta_fine]*K)\n",
    "    return -likelihood_h(Y,eta_crude,eta_fine,theta_fine)\n",
    "\n",
    "# try to load what has been saved\n",
    "try:\n",
    "    with open('../Data/backup_divebomb_norm/hierarchical_params_%d_%d_%d_%d.pkl' % (K,N,crude_timescale,diff_norms), 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "        \n",
    "# if nothing has been saved, find parameters again\n",
    "except FileNotFoundError:\n",
    "    print('No hierarchial parameters found. Optimizing parameters...')\n",
    "    x0 = np.concatenate([eta_crude.ravel(),eta_fine.ravel(),theta_fine.ravel()])\n",
    "    start = time.time()\n",
    "    res = minimize(loss_fn, x0, method = 'Nelder-Mead', options={'disp': True, 'adaptive':True})\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    \n",
    "    with open('../Data/backup_divebomb_norm/hierarchical_params_%d_%d_%d_%d.pkl' % (K,N,crude_timescale,diff_norms), 'wb') as f: \n",
    "        pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# do it again if you are not happy with the current parameters\n",
    "'''\n",
    "def loss_fn(x):\n",
    "    eta_crude = x[:K**2].reshape((K,K))\n",
    "    eta_fine = x[K**2:K**2+K*(N**2)].reshape(K,N,N)\n",
    "    if diff_norms:\n",
    "        theta_fine = np.reshape(x[K**2+K*(N**2):],(K,N,d,d+1))\n",
    "    else:\n",
    "        theta_fine = np.reshape(x[K**2+K*(N**2):],(N,d,d+1))\n",
    "        theta_fine = np.array([theta_fine]*K)\n",
    "    return -likelihood_h(Y,eta_crude,eta_fine,theta_fine)\n",
    "\n",
    "iter_num = 2\n",
    "x0 = np.concatenate([eta_crude.ravel(),eta_fine.ravel(),theta_fine.ravel()])\n",
    "x0 += 0.01*np.random.normal(size=x0.shape)\n",
    "x0 = np.maximum(10e-8,x0)\n",
    "start = time.time()\n",
    "res = minimize(loss_fn, x0, method = 'Nelder-Mead', options={'disp': True, 'adaptive':True})\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "with open('../Data/backup_divebomb_norm/hierarchical_params_%d_%d_%d_%d_%d.pkl' % (K,N,crude_timescale,diff_norms,iter_num), 'wb') as f: \n",
    "    pickle.dump(res, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if diff_norms:\n",
    "    theta_fine = np.reshape(res['x'][K**2+K*(N**2):],(K,N,d,d+1))\n",
    "else:\n",
    "    theta_fine = np.reshape(res['x'][K**2+K*(N**2):],(N,d,d+1))\n",
    "    theta_fine = np.array([theta_fine]*K)\n",
    "    \n",
    "theta_fine = np.array(theta_fine)\n",
    "\n",
    "eta_crude = res['x'][:K**2].reshape((K,K))\n",
    "eta_crude = np.array(eta_crude)\n",
    "np.fill_diagonal(eta_crude, 0)\n",
    "ptm_crude = np.exp(eta_crude)\n",
    "ptm_crude = (ptm_crude.T/np.sum(ptm_crude,1)).T\n",
    "delta_crude = np.ones((1,K))/K\n",
    "for _ in range(10):\n",
    "    delta_crude = delta_crude.dot(ptm_crude)\n",
    "    \n",
    "eta_fine = res['x'][K**2:K**2+K*(N**2)].reshape(K,N,N)\n",
    "eta_fine = eta_fine = np.array(eta_fine)\n",
    "ptm_fine = np.zeros_like(eta_fine)\n",
    "delta_fine = np.ones((K,N))/N\n",
    "for i,ef in enumerate(eta_fine):\n",
    "    ptm_fine[i] = np.exp(ef)\n",
    "    ptm_fine[i] = (ptm_fine[i].T/np.sum(ptm_fine[i],1)).T\n",
    "    for _ in range(10):\n",
    "        delta_fine[i,:] = delta_fine[i,:].dot(ptm_fine[i])\n",
    "\n",
    "print('Crude Probability Transition Matrix:')\n",
    "print(ptm_crude)\n",
    "print('')\n",
    "print('Crude Stationary Distribution:')\n",
    "print(delta_crude)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('Fine Probability Transition Matrices:')\n",
    "print(ptm_fine)\n",
    "print('')\n",
    "print('Fine Stationary Distributions:')\n",
    "print(delta_fine)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('Multivariate Normal Means:')\n",
    "print(theta_fine[0,:,:,0])\n",
    "print('')\n",
    "\n",
    "print('Multivariate Normal Covariances:')\n",
    "for n in range(N):\n",
    "    print(theta_fine[0,n,:,1:].T.dot(theta_fine[0,n,:,1:]))\n",
    "    print('')\n",
    "\n",
    "titles = ['Dive Duration (seconds)','Maximum Depth (meters)','Bottom Variance (meters^2)']\n",
    "xlabels = ['Seconds','Meters','Meters^2']\n",
    "xlims = [[0,300],[0,50],[0,3]]\n",
    "\n",
    "\n",
    "plt.subplots(K,d, figsize= (20,20))\n",
    "\n",
    "for k,theta in enumerate(theta_fine):\n",
    "    for i,state_th in enumerate(theta):\n",
    "        for j,dim_th in enumerate(state_th):\n",
    "\n",
    "            plt.subplot(K,d,k*d+j+1)\n",
    "            \n",
    "            x = np.linspace(xlims[j][0],xlims[j][1],10000)\n",
    "            sig2 = state_th[:,1:].T.dot(state_th[:,1:])\n",
    "            y = norm.pdf(x,dim_th[0],np.sqrt(sig2[j,j]))\n",
    "            plt.plot(x,y)\n",
    "\n",
    "            plt.title(titles[j] + ', Crude State %d' % k)\n",
    "            plt.xlabel(xlabels[j])\n",
    "            plt.ylabel('Probability Density')\n",
    "            plt.legend(['Production State %d' % n for n in range(N)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Viterbi Algorithm for the crude states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Viterbi algorithm on the crude states:\n",
    "\n",
    "def viterbi_crude(Y,ptm_crude,eta_fine,theta_fine):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the MAP estimate of state trajectory of Hidden Markov Model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : array (T,d)\n",
    "        Observation state sequence. int dtype.\n",
    "    ptm : array (N, N)\n",
    "        State transition matrix.\n",
    "    theta : array (N, d, d+1)\n",
    "        Emission parameters for multivariate normal distribution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : array (T,)\n",
    "        Maximum a posteriori probability estimate of hidden state trajectory,\n",
    "        conditioned on observation sequence y under the model parameters A, B,\n",
    "        Pi.\n",
    "    M1: array (K, T)\n",
    "        the probability of the most likely path so far\n",
    "    M2: array (K, T)\n",
    "        the x_j-1 of the most likely path so far\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cardinality of the state space\n",
    "    K = ptm_crude.shape[0]\n",
    "    M = len(Y)\n",
    "    \n",
    "    # get approx stationary dist.\n",
    "    delta = np.ones((1,K))/K\n",
    "    for _ in range(10):\n",
    "        delta = delta.dot(ptm_crude)\n",
    "    \n",
    "    M1 = np.zeros((M,K)) # probs of most likely state\n",
    "    M2 = np.zeros((M,K)) # most likely state\n",
    "    \n",
    "    # Initilaize the tracking tables from first observation\n",
    "    log_p_y0_given_h0 = np.array([likelihood_p([Y[0]],ef,tf) for ef,tf in zip(eta_fine,theta_fine)])\n",
    "    M1[0,:] = np.log(delta) + log_p_y0_given_h0\n",
    "    M2[0,:] = -1\n",
    "    \n",
    "    phi = delta\n",
    "    L_p = 0\n",
    "\n",
    "    # Iterate throught the observations updating the tracking tables\n",
    "    for m in range(1,M):\n",
    "        \n",
    "        log_p_yt_given_ht = np.array([likelihood_p([Y[m]],ef,tf) for ef,tf in zip(eta_fine,theta_fine)])\n",
    "        \n",
    "        for k in range(K):\n",
    "            \n",
    "            M1[m,k] = np.max(M1[m-1,:] + np.log(ptm_crude[:,k]) + log_p_yt_given_ht[k])\n",
    "            M2[m,k] = np.argmax(M1[m-1,:] + np.log(ptm_crude[:,k]))\n",
    "\n",
    "    # Build the output, optimal model trajectory\n",
    "    x = -1 * np.ones(M)\n",
    "    x[-1] = np.argmax(M1[M-1,:])\n",
    "    for m in reversed(range(1, M)):\n",
    "        x[m-1] = M2[m, int(x[m])]\n",
    "\n",
    "    return x, M1, M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first find crude states\n",
    "h_hat = []\n",
    "M1 = []\n",
    "M2 = []\n",
    "for Y0 in Y:\n",
    "    h_hat0,M10,M20 = viterbi_crude(Y0,ptm_crude,eta_fine,theta_fine)\n",
    "    h_hat.append(h_hat0)\n",
    "    M1.append(M10)\n",
    "    M2.append(M20)\n",
    "\n",
    "# then find production states\n",
    "s_hat = []\n",
    "T1 = []\n",
    "T2 = []\n",
    "for ts,Y0 in enumerate(Y):\n",
    "    for m,Ym in enumerate(Y0):\n",
    "        s_hat_m, T1_m, T2_m = viterbi(Ym,ptm_fine[int(h_hat[ts][m])],theta_fine[int(h_hat[ts][m])]) # Note that this is not necesarily the MLE!\n",
    "        s_hat.extend(s_hat_m)\n",
    "        T1.append(T1_m)\n",
    "        T2.append(T2_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dives['predicted_production_state_hierarchical'] = s_hat\n",
    "\n",
    "h_hat_long = []\n",
    "for ts,Y0 in enumerate(Y):\n",
    "    for m,Ym in enumerate(Y0):\n",
    "        h_hat_long.extend([h_hat[ts][m]] * len(Ym))\n",
    "    \n",
    "dives['predicted_internal_state_hierarchical'] = h_hat_long\n",
    "\n",
    "plt.subplots(N*K,d,figsize=(20,20))\n",
    "\n",
    "xlims = [[0,300],[0,50],[0,3]]\n",
    "\n",
    "for i,(theta,h) in enumerate(zip(theta_fine,[float(k) for k in range(K)])):\n",
    "    for j,(state_th,s) in enumerate(zip(theta,[float(n) for n in range(N)])):\n",
    "        for k,(dim_th,col) in enumerate(zip(state_th,dims)):\n",
    "\n",
    "            plt.subplot(N*K, d, i*N*d + j*d + k + 1)\n",
    "\n",
    "            dives[(dives['predicted_production_state_hierarchical'] == s) & \\\n",
    "                  (dives['predicted_internal_state_hierarchical'] == h)][col].hist(density = True)\n",
    "\n",
    "            x = np.linspace(xlims[k][0],xlims[k][1],10000)\n",
    "            \n",
    "            mu = dim_th[0]\n",
    "            sig2 = state_th[:,1:].T.dot(state_th[:,1:])\n",
    "            y = norm.pdf(x,dim_th[0],np.sqrt(sig2[k,k]))\n",
    "            plt.plot(x,y)\n",
    "\n",
    "            plt.title('%s, Crude State %d, Production State %d' % (col,int(h),int(s)))\n",
    "            plt.xlabel(xlabels[k])\n",
    "            plt.ylabel('Probability Density')\n",
    "            plt.legend(['Predicted Density','Observed Density'])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(2*d,1,figsize=(20,50))\n",
    "ylabels = ['Dive Duration (seconds)','Maximum Depth (meters)','Bottom Variance (meters^2)']\n",
    "\n",
    "cmap = ['r','b']\n",
    "colors = [cmap[int(i)] for i in h_hat_long]\n",
    "\n",
    "\n",
    "for i in range(d):\n",
    "\n",
    "    plt.subplot(2*d,1,i+1)\n",
    "    plt.scatter(dives['dive_start'],Y_concat[:,i],c=dives['predicted_internal_state_hierarchical'],s=60)        \n",
    "    plt.plot(dives['dive_start'],Y_concat[:,i],'k--')\n",
    "    plt.ylabel(ylabels[i])\n",
    "    \n",
    "for i in range(d):\n",
    "\n",
    "    plt.subplot(2*d,1,d+i+1)\n",
    "    plt.scatter(dives['dive_start'],Y_concat[:,i],c=dives['predicted_production_state_hierarchical'],cmap='rainbow')        \n",
    "    plt.plot(dives['dive_start'],Y_concat[:,i],'k--')\n",
    "    plt.ylabel(ylabels[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Psuedo-Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psuedoresiduals(Y,eta_crude,eta_fine,theta_fine,diff_norms=False):\n",
    "    \n",
    "    L_LOO = [[np.zeros((Y1.shape[0],1)) for Y1 in Y0] for Y0 in Y]\n",
    "    L_CDF = [[np.zeros(Y1.shape) for Y1 in Y0] for Y0 in Y]\n",
    "    psuedo_res = [[np.zeros(Y1.shape) for Y1 in Y0] for Y0 in Y]\n",
    "    \n",
    "    for ts,Yts in enumerate(Y):\n",
    "\n",
    "        for m,Ym in enumerate(Yts):\n",
    "\n",
    "            for t in range(len(Ym)):\n",
    "                print('time series %d of %d, internal state %d of %d, production state %d of %d' \\\n",
    "                      % (ts+1,len(Y),m+1,len(Yts),t+1,len(Ym)))\n",
    "                L_LOO[ts][m][t,0] = likelihood_h(Y,eta_crude,eta_fine,theta_fine,\n",
    "                                               diff_norms=diff_norms,drop_dive=(ts,m,t),cdf_dim=-1)\n",
    "                for d0 in range(d):\n",
    "                    L_CDF[ts][m][t,d0] = likelihood_h(Y,eta_crude,eta_fine,theta_fine,\n",
    "                                                      diff_norms=diff_norms,drop_dive=(ts,m,t),cdf_dim=d0)\n",
    "                    \n",
    "                    psuedo_res[ts][m][t,d0] = norm.ppf(np.exp(L_CDF[ts][m][t,d0] - L_LOO[ts][m][t,0]))\n",
    "                \n",
    "                if np.isnan(L_LOO[ts][m][t]):\n",
    "                    print('NANS')\n",
    "                    return\n",
    "    \n",
    "    return psuedo_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = psuedoresiduals(Y,eta_crude,eta_fine,theta_fine,diff_norms=diff_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_all = np.vstack([np.vstack([np.vstack(res0) for res0 in res1]) for res1 in res])\n",
    "\n",
    "plt.subplots(d,1,figsize = (10,30))\n",
    "\n",
    "for d0 in range(d):\n",
    "    plt.subplot(d,1,d0+1)\n",
    "    h = plt.hist(res_all[:,d0])\n",
    "    x = np.linspace(-4,4,1000)\n",
    "    plt.plot(x,norm.pdf(x)*len(res_all)*(h[1][-1] - h[1][0])/(len(h[1])-1))\n",
    "    plt.legend(['Theoretical Distribution','Psuedoresiduals'])\n",
    "    plt.ylabel(dims[d0])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res_all[:,0],res_all[:,1],'.')\n",
    "plt.show()\n",
    "plt.plot(res_all[:,0],res_all[:,2],'.')\n",
    "plt.show()\n",
    "plt.plot(res_all[:,1],res_all[:,2],'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dives[dives['predicted_production_state'] == 0].plot(x=dims[1], y=dims[2], style='.')\n",
    "dives[dives['predicted_production_state'] == 1].plot(x=dims[1], y=dims[2], style='.')\n",
    "dives[dives['predicted_production_state'] == 2].plot(x=dims[1], y=dims[2], style='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Procudure step-by-step by second for each dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_small = 2\n",
    "d_small = 2\n",
    "dims_small = ['vz','az']\n",
    "\n",
    "df_1hz = df\n",
    "df_1hz['second'] = (df_1hz['time'] - df_1hz['time'].min())/(np.timedelta64(1,'s'))\n",
    "df_1hz['second'] = df_1hz['second'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1hz = df_1hz.groupby('second').agg({'time':'min',\n",
    "                                       'hr_from_start':'mean',\n",
    "                                       'depth':'mean',\n",
    "                                       'time_series':'mean'}).reset_index()\n",
    "df_1hz['vz'] = (df_1hz['depth'].shift(1) - df_1hz['depth'].shift(-1))/0.2\n",
    "df_1hz['az'] = (df_1hz['depth'].shift(1) - 2*df_1hz['depth'] + df_1hz['depth'].shift(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[2]['time'] = raw_data[2]['time'].apply(lambda x: int(x))\n",
    "df_1hz = df_1hz.rename(columns={'time':'timestamp'}).merge(raw_data[2].groupby('time').mean().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dive_data_exists = False\n",
    "for row,data in dives.iterrows():\n",
    "    dive = df_1hz[(df_1hz['time'] >= data['bottom_start']) & \\\n",
    "                  (df_1hz['time'] <= data['bottom_start'] + data['td_bottom_duration'])]\n",
    "    dive['dive_num'] = row\n",
    "    dive['dive_state'] = data['predicted_production_state_hierarchical']\n",
    "    if dive_data_exists:\n",
    "        dive_data = pd.concat([dive_data,dive])\n",
    "    else:\n",
    "        dive_data = dive\n",
    "        dive_data_exists = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for state in range(N):\n",
    "    # get relevant info from Y\n",
    "    Y = []\n",
    "    gbo = dive_data[dive_data['dive_state'] == state].groupby('dive_num')\n",
    "    for key in gbo.groups:\n",
    "        Y.append(gbo.get_group(key)[dims_small].to_numpy())\n",
    "\n",
    "    # initial guesses for theta\n",
    "    mu0 = 0.001*np.random.normal(size=(N_small,d_small))\n",
    "    sig0 = 0.001*(np.random.normal(size=(N_small,d_small,d_small)))**2\n",
    "    theta = np.zeros(shape=(N_small,d_small,d_small+1))\n",
    "\n",
    "    mu0[:,0] += [0,0] # means of velocity\n",
    "    sig0[:,0,0] += [0.1,0.5] # std of velocity\n",
    "\n",
    "    mu0[:,1] += [0,-0.1] # mean of acceleration\n",
    "    sig0[:,1,1] += [0.05,0.1] # std of acceleration\n",
    "\n",
    "    theta[:,:,0] = mu0\n",
    "    theta[:,:,1:] = sig0\n",
    "\n",
    "    # randomly initialize the ptm\n",
    "    eta = -1.0 + np.random.normal(size=(N_small,N_small))\n",
    "\n",
    "    print(eta)\n",
    "    print(theta)\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(1):\n",
    "        l = likelihood_p(Y,eta,theta)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    print(l)\n",
    "\n",
    "    # do the optimization\n",
    "    def loss_fn(x):\n",
    "        eta = x[:N_small**2].reshape((N_small,N_small))\n",
    "        theta = np.reshape(x[N_small**2:],(N_small,d_small,d_small+1))\n",
    "        return -likelihood_p(Y,eta,theta)\n",
    "\n",
    "    # note there are better and faster ways to do this, notably with the baum-welch algorithm, but\n",
    "    # 5 minutes is okay with me, and the results look alright\n",
    "\n",
    "    # try to load what has been saved\n",
    "    try:\n",
    "        with open('../Data/divebomb_norm/naive_params_small_%d_%d.pkl' % (N_small,state), 'rb') as f:\n",
    "            res.append(pickle.load(f))\n",
    "\n",
    "    # if nothing has been saved, find parameters again\n",
    "    except FileNotFoundError:\n",
    "        x0 = np.concatenate([eta.ravel(),theta.ravel()])\n",
    "        start = time.time()\n",
    "        res.append(minimize(loss_fn, x0, method = 'Nelder-Mead', options={'disp': True, 'adaptive':True}))\n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "\n",
    "        with open('../Data/divebomb_norm/naive_params_small_%d_%d.pkl' % (N_small,state), 'wb') as f: \n",
    "            pickle.dump(res[-1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_to_concat = [None for _ in range(N)]\n",
    "\n",
    "plt.subplots(len(dims_small),N,figsize = (30,20))\n",
    "\n",
    "for state in range(N):\n",
    "    \n",
    "    # get relevant info from Y\n",
    "    Y = []\n",
    "    gbo = dive_data[dive_data['dive_state'] == state].groupby('dive_num')\n",
    "    for key in gbo.groups:\n",
    "        Y.append(gbo.get_group(key)[dims_small].to_numpy())\n",
    "        \n",
    "    eta = res[state]['x'][:N_small**2].reshape((N_small,N_small))\n",
    "    theta = np.reshape(res[state]['x'][N_small**2:],(N_small,d_small,d_small+1))\n",
    "\n",
    "    eta = np.array(eta)\n",
    "    theta = np.array(theta)\n",
    "\n",
    "    np.fill_diagonal(eta, 0)\n",
    "\n",
    "    ptm = np.exp(eta)\n",
    "    ptm = (ptm.T/np.sum(ptm,1)).T\n",
    "\n",
    "    delta = np.ones((1,N_small))/N_small\n",
    "    for _ in range(10):\n",
    "        delta = delta.dot(ptm)\n",
    "\n",
    "    print('Probability Transition Matrix:')\n",
    "    print(ptm)\n",
    "    print('')\n",
    "    print('Stationary Distribution:')\n",
    "    print(delta)\n",
    "    print('')\n",
    "    print('Means:')\n",
    "    print(theta[:,:,0])\n",
    "    print('')\n",
    "    print('Covaraince Matrices:')\n",
    "    for n in range(N_small):\n",
    "        print(theta[n,:,1:].T.dot(theta[n,:,1:]))\n",
    "        print('')\n",
    "\n",
    "    titles = ['Velocity','Acceleration']\n",
    "    xlabels = ['Meters/Second','Meters/Second^2']\n",
    "    xlims = [[-1,1],[-2,2]]\n",
    "\n",
    "    f = 0\n",
    "    for i,state_th in enumerate(theta):\n",
    "\n",
    "        for j,dim_th in enumerate(state_th):\n",
    "            \n",
    "            plt.subplot(len(dims_small),N, N*j + state + 1)\n",
    "            x = np.linspace(xlims[j][0],xlims[j][1],10000)\n",
    "\n",
    "            sig2 = state_th[:,1:].T.dot(state_th[:,1:])\n",
    "            if j == 1:\n",
    "                y = norm.pdf(x,-dim_th[0],np.sqrt(sig2[j,j]))\n",
    "            else:\n",
    "                y = norm.pdf(x,dim_th[0],np.sqrt(sig2[j,j]))\n",
    "            plt.plot(x,y)\n",
    "\n",
    "            plt.title('Distribution of %s for Dive Type %d' % (titles[j],state+1), fontsize = 24)\n",
    "            plt.xlabel(xlabels[j], fontsize = 20)\n",
    "            plt.ylabel('Probability Density', fontsize = 20)\n",
    "            plt.xticks(fontsize = 16)\n",
    "            plt.yticks(fontsize = 16)\n",
    "            plt.legend(['Sub-dive State %d' % (n_s+1) for n_s in range(N_small)], prop={'size': 16})\n",
    "    \n",
    "    # then find production states\n",
    "    s_hat = []\n",
    "    for m,Ym in enumerate(Y):\n",
    "        s_hat_m, T1_m, T2_m = viterbi(Ym,ptm,theta)\n",
    "        s_hat.extend(s_hat_m)\n",
    "        \n",
    "    temp = dive_data[dive_data['dive_state'] == state]\n",
    "    temp['predicted_production_state'] = s_hat\n",
    "    dfs_to_concat[state] = dive_data.merge(temp)\n",
    "    \n",
    "plt.savefig('../Plots/subdive_dists.png')\n",
    "dive_data = pd.concat(dfs_to_concat).sort_values(by='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.888\n",
    "x/(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(N_small*d_small,N,figsize=(30,35))\n",
    "xlims = [[-1,1],[-2,2]]\n",
    "\n",
    "for state in range(N):\n",
    "    \n",
    "    # get relevant info from Y\n",
    "    Y = []\n",
    "    gbo = dive_data[dive_data['dive_state'] == state].groupby('dive_num')\n",
    "    for key in gbo.groups:\n",
    "        Y.append(gbo.get_group(key)[dims_small].to_numpy())\n",
    "        \n",
    "    eta = res[state]['x'][:N_small**2].reshape((N_small,N_small))\n",
    "    theta = np.reshape(res[state]['x'][N_small**2:],(N_small,d_small,d_small+1))\n",
    "\n",
    "    eta = np.array(eta)\n",
    "    theta = np.array(theta)\n",
    "\n",
    "    np.fill_diagonal(eta, 0)\n",
    "\n",
    "    ptm = np.exp(eta)\n",
    "    ptm = (ptm.T/np.sum(ptm,1)).T\n",
    "\n",
    "    delta = np.ones((1,N_small))/N_small\n",
    "    for _ in range(10):\n",
    "        delta = delta.dot(ptm)\n",
    "\n",
    "    titles = ['Velocity','Acceleration']\n",
    "    xlabels = ['Meters/Second','Meters/Second$^2$']\n",
    "\n",
    "    for i,state_th in enumerate(theta):\n",
    "        for j,dim_th in enumerate(state_th):\n",
    "            \n",
    "            plt.subplot(N_small*d_small,N, N*d_small*i + N*j + state+1)\n",
    "            \n",
    "            if j == 0:\n",
    "                plt.hist(dive_data[(dive_data['dive_state'] == state) & \\\n",
    "                         (dive_data['predicted_production_state'] == i)][dims_small[j]],\n",
    "                        density=True)\n",
    "            else:\n",
    "                plt.hist(-dive_data[(dive_data['dive_state'] == state) & \\\n",
    "                         (dive_data['predicted_production_state'] == i)][dims_small[j]],\n",
    "                        density=True)\n",
    "            \n",
    "            x = np.linspace(xlims[j][0],xlims[j][1],10000)\n",
    "            sig2 = state_th[:,1:].T.dot(state_th[:,1:])\n",
    "            if j == 0:\n",
    "                y = norm.pdf(x,dim_th[0],np.sqrt(sig2[j,j]))\n",
    "            else:\n",
    "                y = norm.pdf(x,-dim_th[0],np.sqrt(sig2[j,j]))\n",
    "            plt.plot(x,y)\n",
    "                \n",
    "                \n",
    "            plt.title('%s for Dive Type %d, Sub-Dive State %d' % (titles[j],state+1,i+1),\n",
    "                      fontsize = 24)\n",
    "            plt.xlabel(xlabels[j], fontsize = 20)\n",
    "            plt.ylabel('Probability Density', fontsize = 20)\n",
    "            plt.xticks(fontsize = 16)\n",
    "            plt.yticks(fontsize = 16)\n",
    "            plt.legend(['Predicted Density','Observed Density'], prop={'size':16})\n",
    "    \n",
    "plt.savefig('../Plots/subdive_hists.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dive_data[dive_data['depth'] > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dives_to_plot = 3\n",
    "dives_to_plot = np.array([25,200,267])\n",
    "#dives_to_plot = dive_data[dive_data['dive_state'] == 2.0]['dive_num'].unique()\n",
    "#num_dives_to_plot = len(dives_to_plot)\n",
    "#dives_to_plot = np.random.choice(range(dive_data['dive_num'].max()),num_dives_to_plot,replace=False)\n",
    "\n",
    "plt.subplots(num_dives_to_plot,d_small+1,figsize=(10*(d_small+1),10*num_dives_to_plot))\n",
    "ylabels = ['Velocity $(m/s)$',\n",
    "           'Acceleration $(m/s^2)$']\n",
    "\n",
    "leg_dive = ['Dive Type %d' % (n+1) for n in range(N)]\n",
    "leg_prod = ['Sub-Dive State %d' % (n+1) for n in range(N_small)]\n",
    "\n",
    "for i in range(num_dives_to_plot):\n",
    "    for j in range(d_small):\n",
    "        dive_data0 = dive_data[dive_data['dive_num'] == dives_to_plot[i]]\n",
    "        \n",
    "        if j == 0:\n",
    "            mult = 1\n",
    "        else:\n",
    "            mult = -1\n",
    "            \n",
    "        plt.subplot(d_small+1,num_dives_to_plot,j*(num_dives_to_plot)+i+2)\n",
    "        colors = cm.rainbow(np.linspace(0, 1, N_small))\n",
    "        for k,c in enumerate(colors):\n",
    "            min_time = min(dive_data0['second'])\n",
    "            plt.scatter(dive_data0[dive_data0['predicted_production_state'] == k]['second']-min_time,\n",
    "                        mult*dive_data0[dive_data0['predicted_production_state'] == k][dims_small[j]],\n",
    "                        c=c,s=60)\n",
    "        plt.legend(leg_prod,prop = {'size':20})\n",
    "        plt.plot(dive_data0['second']-min(dive_data0['second']),mult*dive_data0[dims_small[j]],'k--')\n",
    "        plt.ylabel(ylabels[j], fontsize=20)\n",
    "        plt.xlabel('Second After Start', fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.title('%s for Dive %d, Dive State %d' % \\\n",
    "                  (ylabels[j],dives_to_plot[i],dive_data0['dive_state'].mean()),\n",
    "                  fontsize=20)\n",
    "        \n",
    "    plt.subplot(num_dives_to_plot,d_small+1,i*(d_small+1)+1)\n",
    "    colors = cm.rainbow(np.linspace(0, 1, N_small))\n",
    "    for k,c in enumerate(colors):\n",
    "        min_time = min(dive_data0['second'])\n",
    "        plt.scatter(dive_data0[dive_data0['predicted_production_state'] == k]['second']-min_time,\n",
    "                    -dive_data0[dive_data0['predicted_production_state'] == k]['depth'],\n",
    "                    c=c,s=60)\n",
    "    plt.legend(leg_prod,prop = {'size':20})\n",
    "    plt.plot(dive_data0['second']-min(dive_data0['second']),-dive_data0['depth'],'k--')\n",
    "    plt.ylabel('Depth $(m)$',fontsize=20)\n",
    "    plt.xlabel('Second After Start',fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.title('Depth at Bottom for Dive %d, Dive State %d' % \\\n",
    "              (dives_to_plot[i],dive_data0['dive_state'].mean()),\n",
    "              fontsize=20)\n",
    "        \n",
    "plt.savefig('../Plots/viterbi_subdives.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dive_data[dive_data['dive_num'] == 114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
