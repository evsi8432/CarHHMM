%\section{Models and parameter estimation}

Consider a sequence of $T$ curves, where any particular curve $t$ is characterized by a curve-level (or coarse-scale) observation $Y_t$ as well as a sequence of $T^*_t$ within-curve (or fine-scale) observations $Y^*_{t}$. Namely, $Y^*_{t} \equiv \big\{Y^*_{t,1},\ldots,Y^*_{t,T^*_t}\big\}$ is made up of function values from curve $t$. We call the sequence of coarse-scale observations $Y \equiv \big\{Y_1, \ldots, Y_T\big\}$ and the collection of all fine-scale observations $Y^* \equiv \big\{Y^*_1,\ldots,Y^*_T \big\}$. We first detail the structure of a traditional HMM followed by three variations which generalize its base structure. We then show how each of these generalized HMMs can be synthesized to form a wide variety of more complicated models.

%Hidden Markov models describe state-switching Markovian processes in discrete time and will be the core structure of the models we use for $Y$ and $Y^*$. We first detail the structure of a traditional HMM and a variation described by \cite{Lawler:2019} called the conditionally auto-regressive hidden Markov model (CarHMM), which explicitly models auto-correlation between observations. Either the HMM or the CarHMM can be used to model a sequence of observations at one of the two scales: for the coarse scale sequence $Y$ or for each of the fine scale sequences, $Y_1^*,\ldots, Y_T^*$. To simultaneously model $Y$ and $Y^*$, we use the hierarchical hidden Markov model (HHMM) \citep{Barajas:2017,Adam:2019}.
%which is comprised of two distinct,x but related, HMMs that jointly model these time series. 
%Next, we describe how to reduce fine-scale correlation by transforming raw observations into meaningful summary statistics. In particular, we apply a discrete Fourier transform (DFT) to the components of $Y^*$ using a moving window to summarize its periodic behaviour. Finally, we show how each of these models can be synthesized to form a wide variety of new models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{HMMs as a base structure}

An HMM is comprised of a sequence of unobserved states $X \equiv \big\{X_1, \ldots, X_T\big\}$ where the hidden state $X_t$ is associated with the observation $Y_t$. Here we focus on the coarse scale sequence of observations, but an HMM can also be used as a sub-model of the fine-scale sequence for a particular dive. The $Y_t$'s are often referred to as ``emissions'' and the index $t$ typically refers to time. 
The $X_t$'s form a Markov chain and take possible values $1, \ldots, N$. Their distribution is governed by the distribution of the initial state $X_1$ and the $N \times N$ transition probability matrix $\Gamma$, where $\Gamma_{ij} = \Pr(X_{t+1} = j | X_t = i)$. 
%
We assume that $X_1$ follows the chain's stationary distribution, which is denoted by $\delta \in \bbR^N$ with $i^{th}$ component
$\delta_i = \Pr(X_1 = i).$
A Markov chain's stationary distribution is determined by its probability transition matrix via $\delta = \delta \Gamma$ and $\sum_{i=1}^N \delta_i = 1$.
%
The distribution of an emission $Y_t$ depends only on the corresponding state $X_t$ and no other observations or hidden states: the conditional probability density function has the form $p\left(y_t|\{X_1,\ldots, X_T\},\{Y_1,\ldots, Y_T\}/ \{Y_t\}\right) = p(y_t|X_t)$.
%
These conditional distributions are governed by state-dependent parameters. Namely, if $X_t = i$ then the state-dependent parameter is $\theta^{(i)}$ and we denote the conditional distribution of $Y_t$ given $X_t=i$ by its conditional density or probability mass function, denoted $f^{(i)}(\cdot ; \theta^{(i)})$, or sometimes simply $f^{(i)}(\cdot)$.
%
Figure \ref{fig:models}a represents the dependence structure of an HMM.

Using observed emissions, here denoted $y = \{y_1,\ldots,y_T\}$, we can find the maximum likelihood estimates of the parameters $\Gamma$ and $\theta \equiv \{\theta^{(1)},\ldots,\theta^{(N)}\}$. We write the likelihood $\calL_{\text{HMM}}$ using the  well-known \textit{forward algorithm} \citep{Zucchini:2016}:
%
$$\calL_{\text{HMM}}(\theta,\Gamma;y) = \delta P(y_1;\theta) \prod_{t=2}^T \Gamma P(y_t;\theta) \mathbf{1}_N$$
%
where $\mathbf{1}_N$ is an $N$-dimensional column vector of ones and
%
$P(y_t;\theta)$ is an $N \times N$ diagonal matrix with $(i,i)^{th}$ entry  $f^{(i)}(y_t; \theta^{(i)})$.
%

Following \citet{Barajas:2017}, we reparameterize the $N \times N$ transition probability matrix $\Gamma$ such that the entries of the matrix are forced to be non-negative and the rows sum to 1:
%
\[
\Gamma_{ij} = \frac{\exp(\eta_{ij})}{\sum_{k=1}^N \exp(\eta_{ik})}, 
\]
%
where $i,j = 1,\ldots,N$ and $\eta_{ii}$ is set to zero for identifiability. This simplifies likelihood maximization by removing constraints in the optimization problem. For simplicity, we will continue to use $\Gamma$ in our notation, suppressing the reparameterization in terms of $\eta$.

\subsection{The CarHMM and relaxing conditional independence}

A conditionally auto-regressive hidden Markov model, or CarHMM  \citep{Lawler:2019}, is a generalization of an HMM which explicitly models auto-correlation in the observation sequence beyond the correlation induced by the hidden state process. Like a traditional HMM, a CarHMM is made up of a Markov chain of unobserved states $X_1,\ldots,X_T$ that can take on integer values between $1$ and $N$. It also has a transition probability matrix $\Gamma$ and initial distribution $\delta$ equal to the stationary distribution of $\Gamma$. Unlike a traditional HMM, the CarHMM assumes that the distribution of $Y_t$ conditioned on $X_1,\ldots, X_T$ and $Y_1,\ldots, Y_{t-1}$ depends on both $X_t$ and $Y_{t-1}$ rather than only $X_t$. The first emission $Y_1$ is treated as fixed as an initial value which does not depend upon $X_1$. We denote the conditional density of $Y_t$ given $Y_{t-1} = y_{t-1}$ and $X_t=i$ as $f^{(i)}(\cdot | y_{t-1}; \theta^{(i)})$ or simply $f^{(i)}(\cdot | y_{t-1})$. This model is highly general, as $f^{(i)}(\cdot | y_{t-1})$ can be any valid density that depends upon the parameters $\theta^{(i)}$ and the previous observation $Y_{t-1}$. 
As a concrete example, if $Y_t$ is a $K$-dimensional vector with components $Y_t(1),\ldots,Y_t(K)$, consider the component-wise sequences $Y_1(k), \ldots, Y_T(k)$ for $k = 1,\ldots K$. One may assume that these sequences are independent given $X_1,\ldots,X_T$ and that $Y_t(k)$ given $X_t = i$ is Normally distributed with parameters $\theta^{(i)}_k = \{\mu^{(i)}_k,\sigma^{(i)}_k,\phi^{(i)}_k\}$, where:
%
\begin{align}
\label{eqn:carhmm}
\begin{split}
\mathbb{E}(Y_{t}(k)|Y_{t-1}(k) = y_{t-1}(k),X_t=i) &= \phi^{(i)}_k ~ y_{t-1}(k) ~+ ~(1-\phi^{(i)}_k)  ~\mu^{(i)}_k, \\
\mathbb{V}(Y_t(k)| Y_{t-1}(k) = y_{t-1}(k), X_t = i) &= (\sigma^{(i)}_k)^2.
\end{split}
\end{align}
%
A CarHMM which follows Equation (\ref{eqn:carhmm}) can be viewed as a discrete time version of a state-switching Ornstein-Uhlenbeck process \citep{Michelot:2019}. This follows in the same way that an AR(1) process is the discrete-time version of a traditional Ornstein-Uhlenbeck process. Figure \ref{fig:models}b shows a graphical representation of the dependence structure of a general CarHMM. As previously, the likelihood corresponding to a general CarHMM can be easily calculated using the forward algorithm. If $y$ is a vector of observed emissions, then
\begin{equation*}
    \calL_{\text{CarHMM}}(\theta,\Gamma;y) = \delta \prod_{t=2}^T \Gamma P(y_t|y_{t-1};\theta) \mathbf{1}_N,
    \label{eqn:CarHMM_likelihood}
\end{equation*}
where
%
$P(y_t|y_{t-1};\theta)$ is an $N \times N$ diagonal matrix with $(i,i)^{th}$ entry equal to $f^{(i)}(y_t|y_{t-1}; \theta^{(i)})$.

\subsection{The HHMM and incorporating multiple scales}

A hierarchical hidden Markov model, or HHMM, accounts for different scales of correlation by modelling both the coarse-scale process and fine-scale process with either HMMs \citep{Barajas:2017,Adam:2019} or CarHMMs. The coarse-scale model is as defined previously, where $X_1, \ldots, X_T$ make up an unobserved Markov chain with $N$ possible states and $Y_1,\ldots, Y_T$ are the corresponding observations with state-dependent parameters $\theta^{(i)}, i = 1,\ldots,N$.   
%
In the hierarchical setting, however, each state $X_t$ also emits another sequence of fine-scale unobserved states, $X_t^* \equiv \{X_{t,1}^*,\ldots, X_{t,T_t^*}\}$, which in turn emits a sequence of fine-scale observations $Y_t^* \equiv \{Y_{t,1}^*,\ldots, Y_{t,T_t^*}\}$. For each $t$, the fine-scale process $\{X_t^*, Y_t^*\}$ then follows another HMM (or CarHMM) whose parameters depend on the value of $X_t$. If $X_t = i$, then the components of $X_t^*$ are a Markov chain with an $N^{*(i)} \times N^{*(i)}$ transition probability matrix $\Gamma^{*(i)}$ and initial distribution $\delta^{*(i)}$, which we assume is equal to the stationary distribution of the chain. The distribution of $Y_{t,t^*}$ given $Y_{t,t^*-1} = y_{t,t^*-1}$, $X_{t,t^*}=i^*$, and $X_t=i$ is governed by the parameter $\theta^{*(i,i^*)}$ and has density or probability mass function denoted $f^{*(i,i^*)}\left(\cdot|y^*_{t,t^*-1}; \theta^{*(i,i^*)}\right)$ or simply $f^{*(i,i^*)}(\cdot|y^*_{t,t^*-1})$. We denote the fine-scale emission parameter vector corresponding to $X_t=i$ as $\theta^{*(i)}=\left(\theta^{*(i,1)}, \ldots, \theta^{*(i,N^{*(i)})}\right)$. In summary:
%
% this is new
%
\begin{gather*}
    \{Y, X\} \text{ follows (Car)HMM with } \Gamma \in \bbR^{N \times N}, \\
    %
    \{Y_t   | Y_{t-1} = y_{t-1}, X_t = i\} \sim f^{(i)}(\cdot|y_{t-1};\theta^{(i)}), \\
    %
    \{Y^*_t,X^*_t | X_t = i\} \text{ follows (Car)HMM with } \Gamma^{*(i)} \in \bbR^{N^{*(i)} \times N^{*(i)}}, \\
    %
    \{Y^*_{t,t^*} | Y^*_{t,t^*-1} = y^*_{t,t^*-1}, X^*_{t,t^*} = i^*, X_t = i\} \sim f^{*(i,i^*)}(\cdot|y^{*}_{t,t^*-1};\theta^{(i,i^*)}).
\end{gather*}
Given the coarse-scale hidden states $X$, the $T+1$ sets $\{X_1^*, Y_1^*\}, \ldots, \{X_T^*, Y_T^*\}$ and $\{Y_1,\ldots,Y_T\}$ are assumed to be independent of one another. Figure \ref{fig:models}c graphically displays the dependence structure for an HHMM.
%
% end new
%

Forcing certain parameters to be shared can reduce the complexity of the HHMM. For example, in our killer whale case study (see Section \ref{sec:data}), we take $N^{*(i)} = N^*$ for all $i$. We also share the fine-scale emission parameters across coarse-scale hidden states $\left( \text{i.e., } \theta^{*(1,i^*)} = \cdots = \theta^{*(N,i^*)} = \theta^{*(\cdot,i^*)} \text{ for all } i^* = 1, \ldots, N^* \right)$. Coarse-scale hidden states therefore differ only in coarse-scale emission parameters $\theta^{(i)}$ and fine-scale probability transition matrices $\Gamma^{*(i)}$.

Due to the nested structure of the hierarchical hidden Markov model, the likelihood is easily calculated using the forward algorithm.
%
Let $y$ be the $T$-vector of the observed coarse-scale emissions and
$y^* \equiv \left\{y^*_1, \ldots,y^*_T\right\}$ be the collection of $T$ observed fine-scale emission vectors.
%
In addition, let $\theta^* \equiv \{\theta^{*(1)}, \ldots, \theta^{*(N)}\}$ denote the collection of all fine-scale emission parameters and $\Gamma^* \equiv \{\Gamma^{*(1)}, \ldots, \Gamma^{*(N)}\}$ denote the collection of all fine-scale transition probability matrices. The likelihood of the observed data is then
%
\begin{equation}
    \calL_{\text{HHMM}}(\theta,\theta^*,\Gamma,\Gamma^*;y,y^*) = \delta P(y_1,y_1^*;\theta,\theta^*,\Gamma^*) \prod_{t=2}^T \Gamma P(y_t,y_t^*|y_{t-1};\theta,\theta^*,\Gamma^*) \mathbf{1}_N
    \label{eqn:HHMM_likelihood}
\end{equation}
%
where $P(y_1,y_1^*;\theta,\theta^*,\Gamma^*)$ is an $N \times N$ diagonal matrix with $(i,i)^{th}$ entry equal to $f^{(i)}(y_1)\calL_{\text{(Car)HMM}}\left(\theta^{*(i)},
\Gamma^{*(i)};y_1^*\right)$ if the coarse-scale model is an HMM or $\calL_{\text{(Car)HMM}}\left(\theta^{*(i)},
\Gamma^{*(i)};y_1^*\right)$ if the coarse-scale model is a CarHMM. The fine-scale likelihood $\calL_{\text{(Car)HMM}}$ corresponds to the likelihood of the fine-scale model, which can be either a CarHMM or an HMM. In addition, $P(y_t,y_t^*|y_{t-1};\theta,\theta^*,\Gamma^*)$ is an $N \times N$ diagonal matrix with $(i,i)^{th}$ entry equal to 
$f^{(i)}(y_t|y_{t-1})\calL_{\text{HMM}}\left(\theta^{*(i)},
\Gamma^{*(i)};y_t^*\right)$ for both a CarHMM and an HMM.
%
For more information on specific considerations for HHMMs, such as incorporating covariates into the probability transition matrix, state decoding, model selection and model checking, see \citet{Adam:2019}. 

\subsection{The HMM-DFT and transforming fine-scale observations}
\label{subsec:STFT}

In many applications where data are collected at high frequencies, intricate dependency structures may arise within the fine-scale process which cannot be adequately modelled with any of the HMM-based models mentioned thus far. 
%For example, the accelerometer data in Figure \ref{fig:data} sporadically displays clear periodic behaviour. 
To handle these additional fine-scale structures, we recommend replacing $Y_t^* = \{Y^*_{t,1},\ldots,Y^*_{t,T^*_t}\}$ with relevant statistics that summarize any non-Markovian behaviour. To maintain the temporal structure of the fine-scale process, summary statistics can be calculated from a moving window over the elements $Y_t^*$. Subject matter experts are often required to determine the specific summary statistics employed as well as the optimal window size and stride length of the moving window. Larger stride lengths result in a loss of information but also reduce the dimension of the fine-scale process. In addition, setting stride length and window size equal to one another avoids artificial residual correlation arising from overlapping windows. For our case study, we use the discrete Fourier transform (DFT) of a moving forward window of width $h$ and stride $h$ across $Y^*_t$. Namely:
%
\begin{align*}
    DFT\{Y^*_{t,t^*},\ldots, Y^*_{t,t^*+h-1}\}(k) = \sum_{n=0}^{h-1} Y^*_{t,t^*+n}\exp\left(-\frac{i 2\pi}{h} kn \right)
\end{align*}
%
for $t^* = 1,h+1,2h+1,\ldots$ and for $k = 0, 1, \ldots, h-1$. We omit the final window if $t^*+h-1$ exceeds $T^*_t$, and if $Y^*_{t,t^*}$ is a vector then the DFT is taken component-wise. Next, we index each window with $\tilde{t}^*$ and calculate transformed observations $\Z_{t,\tilde{t}^*} \equiv \{\Zone_{t,\tilde{t}^*},\Ztwo_{t,\tilde{t}^*}\}$:
%
\begin{equation}
    \label{eqn:z}
    \Zone_{t,\tilde{t}^*} \equiv \frac{1}{h}\sum_{n=1}^{h}Y^*_{t,h(\tilde{t}^*-1)+n}, \quad \Ztwo_{t,\tilde{t}^*} \equiv \sum_{k=1}^{\tilde{\omega}}\Big|\Big|DFT\{Y^*_{t,h(\tilde{t}^*-1)+1},\ldots, Y^*_{t,h\tilde{t}^*}\}(k)\Big|\Big|^2,
\end{equation}
%
where $\tilde{\omega} \leq h-1$ is a problem-specific tuning parameter corresponding to the maximum recorded frequency within each window. In words, $\Zone_{t,\tilde{t}^*}$ is the average acceleration vector during window $\tilde{t}^*$ and $\Ztwo_{t,\tilde{t}^*}$ is the squared two-norm of the component of the window that can be attributed to frequencies between one and $\tilde{\omega}$ periods per window. %(by the Plancherel theorem). 
More intuitively, $\Ztwo_{t,\tilde{t}^*}$ corresponds to the ``wiggliness" of the acceleration data within dive $t$ and window $\tilde{t}^*$.

The fine-scale HMM must be redefined since $\Z_t$ exists on a coarser scale than $Y^*_t$ itself. In particular, if the length of $\Z_t$ is denoted as $\tilde{T}_t^*$ then there are only $\tilde{T}_t^*$ hidden states associated with $\Z_t = \big\{\Z_{t,1},\ldots, \Z_{t,\tilde{T}^*_t}\big\}$, which we denote as $\tilde{X}^*_t = \big\{\tilde{X}^*_{t,1},\ldots, \tilde{X}^*_{t,\tilde{T}^*_t}\big\}$. The fine-scale probability transition matrices  $\Gamma^{*(i)}$ and probability density functions $f^{*(i,i^*)}$ are then applied directly to $\tilde{X}^*_t$ and $\Z_t$ instead of $X^*_t$ and $Y^*_t$. The likelihood of this model is therefore identical to that of the original HMM, but $Y^*$ is replaced with $\Z$ and $X^*$ is replaced with $\tilde{X^*}$. In order to clearly differentiate models, we refer to an HMM with $\Z$ as observations and $\tilde{X^*}$ as hidden states as an HMM-DFT. Figure \ref{fig:models}d displays the dependence structure for an HMM-DFT.

\subsection{Generalized hierarchical Markov models}

%By definition, an 
Traditional HHMMs treat both the coarse-scale and the fine-scale processes as realizations of an HMM or CarHMM. 
%However, it is straightforward to generalize the coarse-scale HMM by replacing it with a CarHMM, yielding the following likelihood:
%\[
%\calL_{\text{coarse}}(\theta,\theta^*,\Gamma;y,y^*) = \delta \prod_{t=2}^T \Gamma P(y_t,y^*_t|y_{t-1};\theta,\theta^*) \mathbf{1}_N
%\]
%where $P(y_t,y^*_t|y_{t-1};\theta,\theta^*) $ is an $N \times N$ diagonal matrix with $(i,i)^{th}$ entry corresponding to $X_t=i$ and equal to $f^{(i)}\left(y_t|y_{t-1};\theta^{(i)}\right)\calL_{\text{HMM}}\left(\theta^{*(i)};y^*_t\right)$. 
However, the fine-scale observations for a particular dive $Y^*_t$ can be modelled using a large variety of parametric models which admit easy-to-compute likelihoods or penalized likelihoods. In this case, the fine-scale HMM likelihood term $\calL_{\text{(Car)HMM}}$ in Equation (\ref{eqn:HHMM_likelihood}) is replaced by the likelihood of a general fine-scale model, $\calL_{\text{fine}}(\theta^{*(i)};y^*_t)$:
\[
\calL_{\text{coarse}}(\theta,\theta^*,\Gamma;y,y^*) = \delta P(y_1,y^*_1;\theta,\theta^*) \prod_{t=2}^T \Gamma P(y_t,y^*_t;\theta,\theta^*) \mathbf{1}_N
\]
where $P(y_t,y^*_t;\theta,\theta^*) $ is an $N \times N$ diagonal matrix with $(i,i)^{th}$ entry corresponding to $X_t=i$ and equal to $f^{(i)}\left(y_t;\theta^{(i)}\right)\calL_{\text{fine}}\left(\theta^{*(i)};y^*_t\right)$. 
%
Possible candidates for the fine-scale model include any of the models described in the previous subsections (HMM, CarHMM, HHMM, and HHMM-DFT) in addition to many others not described here. For example, \citet{Bebbington:2007} and \citet{Borchers:2013} investigate data sets with count onsets as observations, so they use variations on a Poisson process as their fine-scale model. If the fine-scale model is a simple Poisson process, then the hierarchical structure described here reduces to a Markov-modulated Poisson process \citep{Fischer:1993}.
Further, \citep{Langrock:2018} use B-splines to model the emission distribution of an HMM as well as incorporate covariates. This non-parametric approach admits a penalized likelihood which can easily be incorporated into this hierarchical structure. 
Another class of fine-scale models is the set of continuous time methods such as a continuous-time HMM (CTHMM) \citep{Liu:2015} or a state-switching Ornstein-Uhlenbeck (OU) process \citep{Michelot:2019}. 
A continuous-time HMM (CTHMM) may be appropriate if observations are not equi-spaced in time \citep{Liu:2015}. \citet{Xu:2018} explain biologging accelerometer data by incorporating a CTHMM into a hierarchical model similar to ours. However, they take a longitudinal approach whereas we use a coarse-scale HMM in our case study.

These examples include a few of many fine-scale models that can act as initial building blocks in a practitioner's toolbox to construct increasingly complex hierarchical models. For example, one may construct a three-level HHMM, where three scales of observations are recorded (fine, medium, and coarse-scale) and dependent traditional HMMs are used to model all three levels. A myriad of possible models can be built using this framework, but these models quickly become complicated and computationally expensive to fit. Therefore, models should be constructed with care to achieve an adequate fit of the data while avoiding over-fitting and high computational costs.