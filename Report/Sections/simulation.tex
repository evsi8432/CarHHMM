% !TeX root = ../main.tex

%\section{Simulation Study}
We perform a simulation study based on data generated from the full CarHHMM-DFT as defined in Section \ref{subsec:model_selection} to evaluate each candidate model when the ground-truth is known. The parameters used to generate the data are based on those estimated in the case study (see Table \ref{table:emis_dists_CarHHMM-DFT}), with slight modifications made for simplicity. In particular, we set the number of subdive states to $N^*=2$ and $\Zone_{t,\tilde t^*}$ to a scalar instead of a three dimensional vector. We then fit all four models to the simulated data. Metrics used to evaluate each model include decoding accuracy of hidden states, bias in parameter estimates, empirical standard errors of parameter estimates, and fitting times. To assess the accuracy of uncertainty estimates, we also compare the empirical standard errors of a given model's parameter estimates with the standard errors estimated using the inverse of the observed Fisher information.

\subsection{Simulation procedure}
\label{subsec:data_simulation}

We generate 500 independent data sets using the CarHHMM-DFT as a generative model. Each data set consists of a sequence of 100 curves which we call a sequence of killer whale dives. Each dive can be one of $N=2$ dive types based on a Markov chain with probability transition matrix
%
$$\Gamma = \begin{pmatrix} 0.79 & 0.21 \\ 0.81 & 0.19 \end{pmatrix}.$$
%
Dive duration is Gamma distributed and the coarse-scale emission parameters are 
%
\[\mu^{(1)} = 25.7s, \enspace \sigma^{(1)} = 9.6s, \enspace \mu^{(2)} = 104.6s, \enspace \sigma^{(2)} = 64.7s.\]
%
After generating the dive durations for all 100 dives in a data set, dive $t$ is broken into a sequence of $\tilde T^*_t = \lfloor Y_t/2 \rfloor$ two-second windows, where the last $Y_t - 2 \tilde T^*_t$ seconds of each simulated dive are ignored. Each two-second segment is assigned one of $N^*=2$ behaviours according to a fine-scale Markov chain $\tilde X^*_t \equiv \left\{\tilde X^*_{t,1}, \ldots, \tilde X^*_{t,\tilde T^*_t} \right\}$ with probability transition matrices
%
\[\Gamma^{*(1)} = \begin{pmatrix} 0.68 & 0.32 \\ 0.05 & 0.95 \end{pmatrix} \quad \text{ and } \quad \Gamma^{*(2)} = \begin{pmatrix} 0.86 & 0.14 \\ 0.15 & 0.85 \end{pmatrix}\]
%
for dive types 1 and 2, respectively.
Instead of generating the raw observations $Y^*_{t,t^*}$, we directly simulate the fine-scale transformed observations $\Z_{t,\tilde t^*} = \left\{\Zone_{t,\tilde t^*}, \Ztwo_{t,\tilde t^*}\right\}$. Recall from Section \ref{subsec:model_selection} that we must specify the mean, standard deviation, and auto-correlation parameters corresponding to $\left\{\Zone_{t,1},\ldots,\Zone_{t,\tilde T_t^*}\right\}$ as well as the mean and standard deviation parameters corresponding to $\left\{\Ztwo_{t,1},\ldots,\Ztwo_{t,\tilde T_t^*}\right\}$. We select the following parameters in line with the results from the case study:
%$\Zone_{t,\tilde t^*}$ and $\Ztwo_{t,\tilde t^*}$ are independent when conditioned on all dive types and subdive states. Each observation of acceleration $\Zone_{t,\tilde t^*}$ is Normally-distributed with explicit auto-correlation while each observation of wiggliness $\Ztwo_{t,\tilde t^*}$ is Gamma-distributed without explicit auto-correlation. In particular, we set the distributions of $\Zone_{t,\tilde t^*}$ and $\Ztwo_{t,\tilde t^*}$ with the following parameters:
%
\begin{gather*}
    \mu_A^{*(\cdot,1)} = 0.0 s, \enspace \sigma_A^{*(\cdot,1)} = 0.034s, \enspace \phi_A^{*(\cdot,1)} = 0.98, \\
    %
    \mu_A^{*(\cdot,2)} = 0.0 s, \enspace \sigma_A^{*(\cdot,2)} = 0.079s, \enspace \phi_A^{*(\cdot,2)} = 0.87, \\
    %
    \mu_W^{*(\cdot,1)} = 23.3, \quad \sigma_W^{*(\cdot,1)} = 13.0, \\
    %
    \mu_W^{*(\cdot,2)} = 301.2, \quad \sigma_W^{*(\cdot,2)} = 330.1.
\end{gather*}
%
It is not possible to uniquely reconstruct the raw accelerometer data $Y^*$ from $\Z$ alone, but we describe one possible mapping from $\Z$ to $Y^*$ in the appendix. Figure 21 of the supplementary material shows one realization of $\Z$ for five dives of one simulated data set along with the corresponding reconstructed realization of $Y^*$. 

The two simulated dive types differ in that dives of type 1 are much shorter on average (26 seconds) than dives of type 2 (105 seconds). The two simulated subdive states differ primarily due to $\mu_W^*$ and $\sigma_W^*$ since both are much higher for subdive state 2 than for subdive state 1. These larger parameter values correspond to much more vigorous and variable periodic behaviour in the acceleration data. 

We calculate maximum likelihood estimates $\{\hat \theta, \hat \Gamma, \hat \theta^*, \hat \Gamma^*\}$ for all four candidate models for each of the 500 training data sets using the Cedar Compute Canada cluster with 1 CPU and 4 GB of dedicated memory per model. We then generate a corresponding independent test data set consisting of an additional 100 dives from the generative model with the true parameters $\{\theta, \Gamma, \theta^*, \Gamma^*\}$. 

If the parameters, coarse-scale observations $y$, and fine-scale observations $\z$ are known, then the \textit{forward-backward} algorithm can be used to calculate the hidden state probabilities $\Pr(X_{t} = i|Y=y,\Z=\z)$ and $\Pr(\tilde X^*_{t,t^*} = i^*|Y=y,\Z=\z)$ for all $t$, $t^*$, $i = 1,\ldots,N$, and $i^* = 1,\ldots,N^*$ \citep{Zucchini:2016}. However, since the true parameters rarely known in practice, the maximum likelihood estimates are often used instead to estimate the hidden state probabilities. As such, if $X_t$ is the unknown dive type associated with a given model and $x_t$ is the true simulated dive type of dive $t$, then we define the \textit{average dive decoding accuracy} as the average value of the estimates of $\Pr(X_{t} = x_t|Y=y,\Z=\z)$ across all simulated data sets and dives. Analogously, if $\tilde X^*_{t,\tilde t^*}$ is the unknown subdive state associated with a particular model and $\tilde x^*_{t,\tilde t^*}$ is the true simulated subdive state for dive $t$ and window $\tilde t^*$, then we define the \textit{average subdive decoding accuracy} as the average value of the estimates of $\Pr(\tilde X^*_{t,t^*} = \tilde x^*_{t,\tilde t^*}|Y=y,\Z=\z)$ across all simulated data sets, dives, and windows. We find both the average dive decoding accuracy and the average subdive decoding accuracy using the maximum likelihood estimates for each model and training set, the observations $y$ and $\z$ of each test set, and the forward-backward algorithm.

\subsection{Simulation results}

The full CarHHMM-DFT is the best performing model of the four candidates since it is the generating model. Its average dive decoding accuracy and average subdive decoding accuracy are both greater than 0.9. All parameter estimates of fine-scale mean values and probability transition matrices, $\hat \mu^*$, $\hat \Gamma^*$ and $\hat \Gamma$, respectively, have statistically insignificant biases. In addition, the biases of $\hat \sigma$, $\hat \phi$, and $\hat \mu$ are either as small or smaller than the other three candidate models. The empirical standard errors of all parameter estimates ($\hat \theta$, $\hat \Gamma$, $\hat \theta^*$, $\hat \Gamma^*$) are well-approximated by the inverse of the observed Fisher information matrix, although the estimated standard errors tend to be slightly smaller than the empirical standard errors. This underestimation is especially noticeable for parameters associated with the wiggliness $\Ztwo_{t,\tilde t^*}$, where the empirical standard error can be up to twice as large as the estimated standard error. See Tables 5 through 9 of the supplementary material for detailed results.

The HHMM-DFT performs similarly to the CarHHMM-DFT in most respects. Its average dive decoding accuracy is comparable to the CarHHMM-DFT while its average subdive decoding accuracy is worse by approximately 5 percentage points. Its parameter estimates are comparable to the CarHHMM-DFT with the notable exception that it greatly overestimates $\sigma_A^{*(\cdot,1)}$ and $\sigma_A^{*(\cdot,2)}$. In addition, the estimated standard errors of $\hat \mu_A^{*(\cdot,1)}$, $\hat \mu_A^{*(\cdot,2)}$, $\hat \sigma_A^{*(\cdot,1)}$, and $\hat \sigma_A^{*(\cdot,2)}$ are much smaller than the associated empirical standard errors (see Table 7 of the supplementary material). These results suggest that the estimates of standard deviation can be too large and estimates of standard errors can be too small when auto-correlation is ignored. This finding is consistent with the results of the case study, where the HHMM-DFT produced larger estimates of $\sigma_A^*$ and smaller estimates of standard error compared to the CarHHMM-DFT. When standard errors are underestimated, the associated confidence intervals are too narrow, implying that researchers may be overconfident in their parameter estimates.

The CarHHMM is the worst-performing model in terms of accuracy, as its average dive decoding accuracy is below $0.9$ and its average subdive decoding accuracy is below $0.8$. This result is consistent with expectations because the CarHHMM does not model the ``wiggliness" of the fine-scale process, which is the most distinct difference between the subdive states. In addition to its poor average decoding accuracy, the CarHHMM is also the worst of the four candidate models at estimating parameters. Parameter estimates associated with subdive type 2 ($\theta^{*(\cdot,2)}$) are especially poor. See Section 2 of the supplementary material for more detailed results.

Finally, the CarHMM-DFT is nearly identical to the CarHHMM-DFT in terms of average subdive decoding accuracy, fine-scale parameter biases, and both estimated and empirical standard error for the fine-scale parameter estimates. In addition, the time required to fit the CarHMM-DFT is less than half of that of the other models (see Table \ref{table:accuracy}). However, this model cannot estimate dive type as it lacks any hierarchical structure. The CarHMM-DFT nonetheless fits a (misspecified) single Gamma distribution over the dive duration of all dives. The resulting parameter estimates ($\hat \mu$ and $\hat \sigma$) are highly correlated (See Figure 25 of the supplementary material).

Figures \ref{fig:acc_coarse} and \ref{fig:acc_fine} display one simulated data set as well as the decoded dive types and subdive states associated with each model. The CarHHMM-DFT and CarHMM-DFT produce similar estimates of subdive state while the HHMM-DFT is slightly more likely to predict that a given window corresponds to subdive state 2. As expected, the CarHHMM is the least accurate model when predicting subdive state. The CarHHMM-DFT and HHMM-DFT yield similar estimates of dive type while the CarHHMM is less accurate and even misclassifies the dive type of the second dive. The CarHMM-DFT does not estimate dive type.