% !TeX root = ../main.tex

%\section{Simulation Study}
We perform a simulation study based on data generated from the full CarHHMM-DFT, as defined in Section \ref{subsec:model_selection}, to evaluate each candidate model when the ground truth is known. The parameters used to generate the data are based on those estimated in the case study (see Table \ref{table:emis_dists_CarHHMM-DFT}), with slight modifications made for simplicity. In particular, we set $\Zone_{t,\tilde t^*}$ to a scalar instead of a three-dimensional vector. We then fit all four models to the simulated data. Metrics used to evaluate each model include hidden state decoding accuracy, bias in parameter estimates, empirical standard errors of parameter estimates, and fitting times. To assess the accuracy of uncertainty estimates, we also compare the empirical standard errors of a given model's parameter estimates with the standard errors estimated using the inverse of the observed Fisher information.

\subsection{Simulation procedure}
\label{subsec:data_simulation}

We generate 500 independent training data sets using the CarHHMM-DFT as a generative model. Each training data set consists of a sequence of 100 curves which we call a sequence of killer whale dives. Each dive can be one of $N=2$ dive types based on a Markov chain with transition probability matrix

\[\Gamma = \begin{pmatrix} 0.847 & 0.153 \\ 0.914 & 0.086 \end{pmatrix}.\]
%
Dive duration is gamma distributed and the coarse-scale emission parameters are $\mu^{(1)} = 27.34$ $s$, $\sigma^{(1)} = 10.96$ $s$, $\mu^{(2)} = 127.55$ $s$, and $\sigma^{(2)} = 63.89$ $s$.
%
After generating the dive durations for all 100 dives in a data set, dive $t$ is broken into a sequence of $\tilde T^*_t = \lfloor Y_t/2 \rfloor$ two-second windows, where the last $Y_t - 2 \tilde T^*_t$ seconds of each simulated dive are ignored. Each two-second segment is assigned one of $N^*=3$ behaviours according to a fine-scale Markov chain $\tilde X^*_t = \big\{\tilde X^*_{t,1}, \ldots, \tilde X^*_{t,\tilde T^*_t} \big\}$ with transition probability matrices

\[\Gamma^{*(1)} = \begin{pmatrix} 0.745 & 0.253 & 0.002 \\ 0.080 & 0.868 & 0.052 \\ 0.000 & 0.229 & 0.771 \end{pmatrix} \quad \text{ and } \quad \Gamma^{*(2)} = \begin{pmatrix} 0.886 & 0.139 & 0.000 \\ 0.150 & 0.815 & 0.035 \\ 0.000 & 0.225 & 0.775 \end{pmatrix}\]
%
for dive types 1 and 2, respectively.
Instead of generating the raw observations $Y^*_{t,t^*}$, we directly simulate the fine-scale transformed observations $\Z_{t,\tilde t^*} = \big\{\Zone_{t,\tilde t^*}, \Ztwo_{t,\tilde t^*}\big\}$. Recall from Section \ref{subsec:model_selection} that we must specify the mean, standard deviation, and autocorrelation parameters corresponding to $\big\{\Zone_{t,1},\ldots,\Zone_{t,\tilde T_t^*}\big\}$ as well as the mean and standard deviation parameters corresponding to $\big\{\Ztwo_{t,1},\ldots,\Ztwo_{t,\tilde T_t^*}\big\}$. We select the following parameters in line with the results from the case study:

\begin{enumerate}
    \item $\mu_A^{*(\cdot,1)} = 0.0$ $s$, $\sigma_A^{*(\cdot,1)} = 0.05$ $s$, $\phi_A^{*(\cdot,1)} = 0.97$, $\mu_W^{*(\cdot,1)} = 34.01$, and $\sigma_W^{*(\cdot,1)} = 22.99$.
    %
    \item $\mu_A^{*(\cdot,2)} = 0.1$ $s$, $\sigma_A^{*(\cdot,2)} = 0.1$ $s$, $\phi_A^{*(\cdot,2)} = 0.83$, $\mu_W^{*(\cdot,2)} = 490.06$, and $\sigma_W^{*(\cdot,2)} = 502.56$.
    %
    \item $\mu_A^{*(\cdot,3)} = 0.2$ $s$, $\sigma_A^{*(\cdot,3)} = 0.3$ $s$, $\phi_A^{*(\cdot,3)} = 0.61$, $\mu_W^{*(\cdot,3)} = 9154.16$, and $\sigma_W^{*(\cdot,3)} = 13538.75$.
\end{enumerate}
%
It is not possible to uniquely reconstruct the raw accelerometer data $Y^*$ from $\Z$ alone, but we describe one possible mapping from $\Z$ to $Y^*$ in the appendix. Figure 1 of Supplement B shows one realization of $\Z$ for five dives of one simulated data set along with the corresponding reconstructed realizations of $Y^*$. 

The two simulated dive types differ in that dives of type 1 are much shorter on average (27 $s$) than dives of type 2 (128 $s$). The three simulated subdive states differ primarily because $\mu_W^*$ and $\sigma_W^*$ are much higher for subdive state 3 than for subdive state 2, which in turn are much higher than for subdive state 1. These larger parameter values correspond to much more vigorous and variable periodic behaviour in the acceleration data. 

We calculate the maximum likelihood estimates $\{\hat \theta, \hat \Gamma, \hat \theta^*, \hat \Gamma^*\}$ for all four candidate models for each of the 500 data sets using an optimization procedure similar to that in the case study.

For each of the 500 training data sets, we simulate a test data set to assess how well each model predicts the hidden states as follows. Each test data set consists of a sequence of 100 dives and is created from the generative model with the true parameters $\{\theta, \Gamma, \theta^*, \Gamma^*\}$. To assess the coarse-scale hidden state prediction, we estimate $p_t(i \mid y,\z) = \Pr(X_t=i \mid Y=y,\Z=\z)$ for $i=1,2$ and $t=1,\ldots,100$ using the test set observations $(y,\z)$ and the training set maximum likelihood estimates. These estimates are found using the forward--backward algorithm \citep{Zucchini:2016}. We compare these estimated conditional probabilities to $\{x_1,\ldots,x_{100}\}$, the true coarse-scale state realizations in the test data, by calculating the average dive decoding accuracy for a single training/test data set pair, $\sum_{t=1}^{100} \hat{p}_t(x_t \mid y,\z)/100$. We then report the average of this measure over the 500 training/test data set pairs. 
Analogously, to assess fine-scale state prediction, we estimate $p^*_{t,\tilde t^*}(i^* \mid y,\z) = \Pr(\tilde X^*_{t,\tilde t^*}=i \mid Y=y,\Z=\z)$ for $i^*=1,2,3$, $\tilde t^* = 1,\ldots,\tilde T^*_t$, and $t=1,\ldots,100$ using the test set observations, the training set maximum likelihood estimates, and the forward--backward algorithm. Denoting the true fine-scale state realizations from the test data set by $\{\tilde x^*_{t,1},\ldots,\tilde x^*_{t,\tilde T^*_t}\}$, we define the overall average subdive decoding accuracy as the average value of $\hat{p}^*_{t,\tilde t^*}(\tilde x^*_{t,\tilde t^*} \mid y,\z)$ across all simulated test data sets, dives, and windows. The conditional probabilities are estimated according to each of the four models under study using the maximum likelihood estimates from the training data set in conjunction with the forward--backward algorithm.

\subsection{Simulation results}

The full CarHHMM-DFT is the best-performing model of the four candidates since it is the generating model. Its average dive decoding accuracy is approximately $0.96$ and its average subdive decoding accuracy is approximately $0.91$. 
All parameter estimates of emission distributions ($\theta$ and $\theta^*$) and transition probability matrices ($\Gamma$ and $\Gamma^*$) on both the coarse scale and fine scale are either comparable or favourable relative to all other models. The empirical standard errors of all parameter estimates ($\hat \theta$, $\hat \Gamma$, $\hat \theta^*$, and $\hat \Gamma^*$) are well approximated by the inverse of the observed Fisher information matrix, although the estimated standard errors tend to be slightly smaller than the empirical standard errors. This underestimation is especially noticeable for parameters associated with the ``wiggliness" $\Ztwo_{t,\tilde t^*}$, where the empirical standard error can be up to five times as large as the estimated standard error. See Tables 2--6 of Supplement B for detailed results.

The HHMM-DFT has an average dive decoding accuracy comparable to that of the CarHHMM-DFT ($0.96$), but its average subdive decoding accuracy is worse by approximately seven percentage points ($0.84$). The HHMM-DFT's parameter estimates are comparable to the CarHHMM-DFT with the notable exception that the former greatly overestimates $\sigma_A^{*(\cdot,1)}$ and $\sigma_A^{*(\cdot,2)}$ and slightly overestimates $\sigma_A^{*(\cdot,3)}$. In addition, the estimated standard errors of $\hat \mu_A^{*(\cdot,1)}$, $\hat \mu_A^{*(\cdot,2)}$, $\hat \mu_A^{*(\cdot,3)}$, $\hat \sigma_A^{*(\cdot,1)}$, $\hat \sigma_A^{*(\cdot,2)}$, and $\hat \sigma_A^{*(\cdot,3)}$ are much smaller than the associated empirical standard errors (see Table 3 of Supplement B). These results suggest that estimates of standard deviation can be too large and estimates of standard errors can be too small when autocorrelation is ignored. This finding is consistent with the results of the case study, where the HHMM-DFT produce larger estimates of $\sigma_A^*$ and smaller estimates of standard error compared to the CarHHMM-DFT. When standard errors are underestimated, the associated confidence intervals are too narrow, and so researchers may be overconfident in their parameter estimates.

The CarHHMM is the worst-performing model in terms of its dive decoding accuracy, which is approximately four percentage points worse than that of the CarHHMM-DFT ($0.92$). The former's average subdive decoding accuracy is approximately six percentage points worse than the CarHHMM-DFT ($0.85$). This result is consistent with our expectations because the CarHHMM does not model the ``wiggliness" of the fine-scale process, which is the most distinct difference between the subdive states. In addition to its relatively poor average decoding accuracy, the CarHHMM is also the worst of the four candidate models at estimating emission parameters and transition probability matrices. Estimates associated with subdive states 2 and 3 ($\theta^{*(\cdot,2)}$ and $\theta^{*(\cdot,3)}$) are especially poor. See Supplement B for more-detailed results.

Finally, the CarHMM-DFT is nearly identical to the CarHHMM-DFT in terms of average subdive decoding accuracy, fine-scale parameter biases, and both estimated and empirical standard error for the fine-scale parameter estimates. In addition, the time required to fit the CarHMM-DFT is less than one-third that of the other models (see Table \ref{table:accuracy}). However, this model cannot differentiate between dive types as it assumes that there is only one. The CarHMM-DFT nonetheless fits a (misspecified) single gamma distribution over the dive duration of all dives. The resulting parameter estimates ($\hat \mu_Y$ and $\hat \sigma_Y$) are highly correlated (See Figure 5 of Supplement B).

Figures \ref{fig:acc_coarse} and \ref{fig:acc_fine} display five dives of one simulated data set as well as the decoded dive types and subdive states associated with each model. The CarHHMM-DFT and CarHMM-DFT produce similar estimates of subdive state, the HHMM-DFT is slightly more likely to misclassify subdive state 1, and the CarHHMM is more likely to misclassify subdive state 3. All models classify dive type with high accuracy, with the exception of the CarHMM-DFT, which does not estimate dive type.