% !TeX root = ../main.tex

\addcontentsline{toc}{section}{Appendices}
\renewcommand*{\thesubsection}{\Alph{subsection}}

\section*{Appendix}

\setcounter{subsection}{0}

\subsection{Detailed Description of Data Simulation}

$\hat{Y}^*_{t,s^*}$ was simulated using the following procedure. Note that the $k^{th}$ Fourier mode of $\hat{Y}^*_{t,s^*}$ is denoted as $\hat{Y}^{*(k)}_{t,s^*}$:

\begin{align*}
	(\hat{Y}^{*(0)}_{t,0}|X^*_{t,0} = i) &\sim \mathcal{N} \left(0, \sigma^{*(i)} \right) & \\
	%
	(\hat{Y}^{*(0)}_{t,s^*}|X^*_{t,s^*} = i) &\sim \mathcal{N} \left(\phi^{*(i)} * \hat{Y}^{*(0)}_{t,s^*-1}, \sigma^{*(i)} \right), & s^* = 1,2,\ldots, \lfloor Y_t/2 \rfloor \\
	%
	\hat{Y}^{*(k)}_{t,s^*} &= a_{t,s^*}^{(k)} i\sqrt{b^{(k)}_{t,s^*}}, & k = 1,\ldots,49 \\
\end{align*}
\begin{align*}
    a_{t,s^*}^{(k)} &\sim  \left\{\begin{array}{lr}
	-1 & \quad w.p. \enspace 1/2 \\
	1  & \quad w.p. \enspace 1/2
	\end{array}\right. \\
	%
	(b^{(k)}_{t,s^*}|X^*_{t,s^*}  = 1) &\sim {\rm{Gamma}}(1/k^2, 1) \\
	%
	(b^{(k)}_{t,s^*}|X^*_{t,s^*} = 2) &\sim \left\{\begin{array}{lr}
	{\rm{Gamma}}(1/k^2, 1), \quad & k \notin \{1,2\} \\
	{\rm{Gamma}}(100,1), & k = 1 \\
	{\rm{Gamma}}(50,1), & k = 2
	\end{array}\right. 
\end{align*}

\begin{align*}
    \hat{Y}^{*(50)}_{t,s^*} &= 0 & \\
	%
	\hat{Y}^{*(k)}_{t,s^*}  &= -\hat{Y}^{*(100-n)}_{t,s^*}, & \qquad k = 51,\ldots,99
\end{align*}

$Y^*_{t,(100s^*+1):(100s^*+100)}$ was set using the inverse discrete Fourier transform of $\hat{Y}^*_{t,s^*}$:

$$Y^*_{t,(100s^*+1):(100s^*+100)} = IDFT\left(\hat{Y}^*_{t,s^*}\right), \qquad s^* = 0,\ldots,\lfloor Y_t/2 \rfloor - 1$$

There are several practical reasons behind this construction. First, $\hat{Y}^*_{t,s^*}$ is anti-symmetric about $\hat{Y}^{*(50)}_{t,s^*}$ so that its inverse Fourier transform is real-valued. $\hat{Y}^{*(k)}_{t,s^*}$ also decays like $1/k^2$ so that $Y^*_{t,t^*}$ remains continuous within a 2-second window. Note, however, that $Y^*_{t,s^*}$ is not continuous \textit{between} windows, as can be seen in (fig \ref{fig:fourier_example}). However, the jumps are not too severe since $\hat{Y}^{*(0)}_{t,s^*}$ and $\hat{Y}^{*(0)}_{t,s^*+1}$ are highly correlated. See (fig. \ref{fig:sim_data}) for details.

From here it is straightforward to calculate both $Z^{*(1)}$ and $Z^{*(2)}$ after setting $\tilde{f}$. We pick $\tilde{f} = 10$ periods per window, or 5 hertz. Note that $Z^{*(1)}_{t,s^*} = \mathcal{R}\left(\hat{Y}^{(0)}_{t,s^*}\right)$, so selecting $\sigma^{*(i)}$ and $\phi^{*(1)}$ appropriately will ensure the distributions from (eqn \ref{}). 

Note that $Z^{*(2)}$, is the sum of gamma distributions with the same scale parameter, so the distribution of $Z^{*(2)}$ is also a Gamma distribution:

$$Z^{*(2)} = \sum_{k=1}^{10} b^{(k)}_{t,s^*}$$
$$Z^{*(2)} \sim {\rm{Gamma}}


























\iffalse


\subsection{Dimension Reduction on $\hat{Y_t^*}$}

First, we simply use down-sampling and only record every $w^{th}$ time step of $\hat{Y}_t^*$. This both reduces the space of $\hat{Y}_t^*$ to $\mathbb{C}^{\lfloor S^*_t / w \rfloor \times w}$ and ensures that none of the sliding windows overlap when taking the STFT. This is important because HMMs assume temporal independence between observations. Next, the dimension of $\hat{Y}_t^*$ can be cut in half by recognizing that the $Y_t^*$ is real-valued, and therefore $\hat{Y}_{t,k}^*$ is equal to the complex-congugate of $\hat{Y}_{t,w-k-1}^*$. Finally, by Parseval's Thereom we have that:

$$\sum_{n = 0}^{w-1} |Y^*_{t+n}|^2 = \sum_{n = 0}^{w-1} |\hat{Y}^*_{t,n}|^2$$

\subsection{Equivalency of CarHMM and one-dimensional state-switching Ornstein-Uhlenbeck process}

If it is the case that (1) the underlying behavioral state of the continuous-time model must follow a Markov chain rather than a Markov process, and (2) the emission distributions of the CarHMM are gaussian, then the CarHMM and the state-switching continuous model are equivalent. This allows the theoretically grounded continuous-time state-switching model to be used in the computational convieneint HMM (and therefore HHMM) framework. In addition, it gives new intpretation to the learned parameters of the CarHMM in the context of an Ornstein-Uhlenbeck process.

A one-dimensional state-switching Ornstein-Uhlenbeck process $y^*$ is the solution to the following stochastic differential equation:
%
$$dy^*_t = \beta_{x^*_t}(\gamma_{x^*_t} - y^*_t)dt + \omega_{x^*_t} dW_t$$
%
where $x^*_t$ is the fine-scale behavior of the animal at time $t$, $\beta_{x^*_t}$ relates to rate at which the process returns to its mean value, $\gamma_{x^*_t}$ is the long-term mean value of the process, $\omega_{x^*_t}$ is related to short-term variance, and $W$ is a Wiener process. As before, $x^*_t$ is described by an unobserved Markov process. The solution to this equation is known to be the following \cite{Michelot:2019}:
\begin{align*}
y^*_{t+\delta} \sim \mathcal{N}\left((1-e^{-\beta_{x^*_t}\delta})\gamma_{x^*_t} + e^{-\beta_{x^*_t}\delta} y^*_t,\quad \frac{\omega_{x^*_t}^2}{2\beta_{x^*_t}} (1-e^{-2\beta_{x^*_t}\delta})\right)
\end{align*}
Now, suppose that $\delta$ is constant for all observations, as is the case for hidden Markov models. In addition, introduce the following transformations:
\begin{align*}
\mu_{x^*_t} = \gamma_{x^*_t}, \qquad \phi_{x^*_t} = e^{-\beta_{x^*_t}\delta}, \qquad \sigma^2_{x^*_t} = \frac{\omega_{x^*_t}^2}{2\beta_{x^*_t}} (1-e^{-2\beta_{x^*_t}\delta})
\end{align*}
Then, we have the following:
\begin{align*}
y^*_{t+\delta} \sim \mathcal{N}\left((1-\phi_{x^*_t})\mu_{x^*_t} + \phi_{x^*_t} y^*_t,\quad \sigma_{x^*_t}^2 \right)
\end{align*}
%
If $\delta$ is fixed and $x^*_t$ is adjusted to follow a Markov chain rather than a Markov process, then this model is equivalent to the CarHMM with normal emission probabilities. Note that all of the parameter transformations above are one-to-one, so it is easy to go from the CarHMM to the continuous model and back again. This allows for the principled construction of the continuous-time model to be combined with the computational convenience of the CarHMM.

\fi
