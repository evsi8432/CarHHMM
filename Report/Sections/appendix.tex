% !TeX root = ../main.tex

\addcontentsline{toc}{section}{Appendices}
\renewcommand*{\thesubsection}{\Alph{subsection}}

\section*{Appendix}

\setcounter{subsection}{0}

\section{Results regarding CarHMMs and the Ornstein-Uhlenbeck process}

\subsection{Continuous Time models}

Although the CarHMM can incorporate auto-correlation into the structure of an HMM, they break down when observations are taken at irregular time intervals since the probability transition matrix $\Gamma$ would then have to depend upon the time step between observations. A common solution to this issue is to use a continuous-time method such as the one introduced by Michelot et al \citep{Michelot:2019}, which models the movement of an animal as an Ornstein-Uhlenbeck process with parameters that depend upon the underlying behavioural state of the animal. Namely, a state-switching Ornstein-Uhlenbeck process $Y$ is the solution to the following stochastic differential equation:
%
\begin{equation}
    \label{eqn:SDE}
    dY_t = \beta^{(X_t)}(\gamma^{(X_t)} - Y_t)dt + \omega^{(X_t)} dW_t
\end{equation}
%
where $X_t$ follows some stochastic process which defines the hidden behaviour of the animal at time $t$, $\beta^{(X_t)}$ relates to rate at which the process returns to its mean value, $\gamma^{(X_t)}$ is the long-term mean value of the process, $\omega^{(X_t)}$ is related to short-term variance, and $W$ is a Wiener process. One notable difference between the state-switching OU process and the CarHMM is that the time index $t \in \mathbb{R}$ exists in continuous time and is not necessarily an integer. If the behavioral state $X_t$ is known and does not change between observations, the solution to (eqn \ref{eqn:SDE}) is known to be the following \citep{Michelot:2019}:
%
\begin{align}
Y_{t+\Delta t} | X_{t} \sim \mathcal{N}\left((1-e^{-\beta^{(X_t)}\Delta t})\gamma^{(X_t)} + e^{-\beta^{(X_t)}\Delta t} Y_t,\quad \frac{\omega^{(X_t)^2}}{2\beta^{(X_t)}} (1-e^{-2\beta^{(X_t)}\Delta t})\right)
\label{eqn:OU_sol}
\end{align}
%
where $\Delta t$ is the time difference between any two observations $Y_t$ and $Y_{t+\Delta t}$. Continuous time models can be built up from arbitrarily complex stochastic differential equations and allow for uneven step lengths in the observations sequence $Y$. However, most continuous time models require MCMC algorithms to perform inference and as a result are not easily incorporated into the HHMM structure.

\subsection{Equivalence of CarHMM and OU process}

Although most continuous time models are difficult to incorporate into an HHMM, we show in (thrm 1) that under certain conditions, the CarHMM is equivalent to a state-switching Ornstein-Uhlenbeck process shown in (eqn \ref{eqn:OU_sol}). This gives new interpretation to the learned parameters of the CarHMM in the context of a continuous-time model.

\begin{theorem}{Theorem 1.}{}%
If:
\begin{enumerate}
    \item The hidden behavioural process $X$ from (Eq. \ref{eqn:SDE}) follows a Markov chain with $N$ possible states and transitions occur at equi-spaced time stamps $\left(\Delta t, \ldots, (T-1)\Delta t\right)$, and:
    \item Observations of the SDE from (Eq. \ref{eqn:SDE}) are taken at times $\left(0, \Delta t, \ldots, (T-1)\Delta t\right)$,
\end{enumerate}
then the observations $y$ are equivalent to the output of a conditionally auto-regressive hidden Markov model with normal emission distributions and parameters $\Theta = (\theta^{(1)}, \ldots, \theta^{(N)}); \enspace \theta^{(i)} = \{\mu^{(i)},\sigma^{(i)},\phi^{(i)}\}$, where:

\begin{align}
\mu^{(i)} = \gamma^{(i)}, \qquad \sigma^{(i)} = \sqrt{\frac{\omega^{(i)^2}}{2\beta^{(i)}} (1-e^{-2\beta^{(i)}\Delta t})}, \qquad \phi^{(i)} = e^{-\beta^{(i)}\Delta t} \label{eqn:CarHMM_to_OU}
\end{align}

\end{theorem}

\begin{proof}{Proof}{}%
Combining equation (\ref{eqn:CarHMM_to_OU}) with equation (\ref{eqn:OU_sol}) gives:
%
\begin{align*}
\left(Y_{s \Delta t} | X_{(s-1)\Delta t} = i \right) \sim \mathcal{N}\left((1-\phi^{(i)}) \mu^{(i)} + \phi^{(i)} Y_{(s-1) \Delta t}, \enspace \sigma^{2(i)} \right),\\
s = 1, \ldots, T-1
\end{align*}
%
If $X_t$ follows a Markov chain with transitions at the observation times, then the behavioural state $X_t$ does not change between observations and we can re-index $Y_{(s-1) \Delta t} = Y'_s$ and $X_{(s-2)\Delta t: (s-1) \Delta t} = X'_s$ for $s = 2,\ldots T$, yielding the desired result:

\begin{align*}
\left(Y'_s| X'_s = i \right) \sim \mathcal{N}\left((1-\phi^{(i)}) \mu^{(i)} + \phi^{(i)} Y'_{s-1}, \enspace \sigma^{2(i)} \right)\\
s = 2, \ldots, T
\end{align*}
%
$X'$ follows a Markov chain, and the distribution of $(Y'_s|X'_s,Y'_{s+1})$ is consistent with that of a CarHMM with normal emission distributions.
\end{proof}


\subsection{Detailed Description of Data Simulation}

$\hat{Y}^*_{t,s^*}$ was simulated using the following procedure. Note that the $k^{th}$ Fourier mode of $\hat{Y}^*_{t,s^*}$ is denoted as $\hat{Y}^{*(k)}_{t,s^*}$:

\begin{align*}
	(\hat{Y}^{*(0)}_{t,0}|X^*_{t,0} = i) &\sim \mathcal{N} \left(0, \sigma^{*(i)} \right) & \\
	%
	(\hat{Y}^{*(0)}_{t,s^*}|X^*_{t,s^*} = i) &\sim \mathcal{N} \left(\phi^{*(i)} * \hat{Y}^{*(0)}_{t,s^*-1}, \sigma^{*(i)} \right), & s^* = 1,2,\ldots, \lfloor Y_t/2 \rfloor \\
	%
	\hat{Y}^{*(k)}_{t,s^*} &= a_{t,s^*}^{(k)} i\sqrt{b^{(k)}_{t,s^*}}, & k = 1,\ldots,49 \\
\end{align*}
\begin{align*}
    a_{t,s^*}^{(k)} &\sim  \left\{\begin{array}{lr}
	-1 & \quad w.p. \enspace 1/2 \\
	1  & \quad w.p. \enspace 1/2
	\end{array}\right. \\
	%
	(b^{(k)}_{t,s^*}|X^*_{t,s^*}  = 1) &\sim {\rm{Gamma}}(1/k^2, 1) \\
	%
	(b^{(k)}_{t,s^*}|X^*_{t,s^*} = 2) &\sim \left\{\begin{array}{lr}
	{\rm{Gamma}}(1/k^2, 1), \quad & k \notin \{1,2\} \\
	{\rm{Gamma}}(100,1), & k = 1 \\
	{\rm{Gamma}}(50,1), & k = 2
	\end{array}\right. 
\end{align*}

\begin{align*}
    \hat{Y}^{*(50)}_{t,s^*} &= 0 & \\
	%
	\hat{Y}^{*(k)}_{t,s^*}  &= -\hat{Y}^{*(100-n)}_{t,s^*}, & \qquad k = 51,\ldots,99
\end{align*}

$Y^*_{t,(100s^*+1):(100s^*+100)}$ was set using the inverse discrete Fourier transform of $\hat{Y}^*_{t,s^*}$:

$$Y^*_{t,(100s^*+1):(100s^*+100)} = IDFT\left(\hat{Y}^*_{t,s^*}\right), \qquad s^* = 0,\ldots,\lfloor Y_t/2 \rfloor - 1$$

There are several practical reasons behind this construction. First, $\hat{Y}^*_{t,s^*}$ is anti-symmetric about $\hat{Y}^{*(50)}_{t,s^*}$ so that its inverse Fourier transform is real-valued. $\hat{Y}^{*(k)}_{t,s^*}$ also decays like $1/k^2$ so that $Y^*_{t,t^*}$ remains continuous within a 2-second window. Note, however, that $Y^*_{t,s^*}$ is not continuous \textit{between} windows, as can be seen in (fig \ref{fig:fourier_example}). However, the jumps are not too severe since $\hat{Y}^{*(0)}_{t,s^*}$ and $\hat{Y}^{*(0)}_{t,s^*+1}$ are highly correlated. See (fig. \ref{fig:sim_data}) for details.

From here it is straightforward to calculate both $Z^{*(1)}$ and $Z^{*(2)}$ after setting $\tilde{f}$. We pick $\tilde{f} = 10$ periods per window, or 5 hertz. Note that $Z^{*(1)}_{t,s^*} = \mathcal{R}\left(\hat{Y}^{(0)}_{t,s^*}\right)$, so selecting $\sigma^{*(i)}$ and $\phi^{*(1)}$ appropriately will ensure correct distribution. 

Note that $Z^{*(2)}$, is the sum of gamma distributions with the same scale parameter, so the distribution of $Z^{*(2)}$ is also a Gamma distribution:

$$Z^{*(2)} = \sum_{k=1}^{10} b^{(k)}_{t,s^*}$$
$$Z^{*(2)} \sim {\rm{Gamma}}$$


























\iffalse


\subsection{Dimension Reduction on $\hat{Y_t^*}$}

First, we simply use down-sampling and only record every $w^{th}$ time step of $\hat{Y}_t^*$. This both reduces the space of $\hat{Y}_t^*$ to $\mathbb{C}^{\lfloor S^*_t / w \rfloor \times w}$ and ensures that none of the sliding windows overlap when taking the STFT. This is important because HMMs assume temporal independence between observations. Next, the dimension of $\hat{Y}_t^*$ can be cut in half by recognizing that the $Y_t^*$ is real-valued, and therefore $\hat{Y}_{t,k}^*$ is equal to the complex-congugate of $\hat{Y}_{t,w-k-1}^*$. Finally, by Parseval's Thereom we have that:

$$\sum_{n = 0}^{w-1} |Y^*_{t+n}|^2 = \sum_{n = 0}^{w-1} |\hat{Y}^*_{t,n}|^2$$

\subsection{Equivalency of CarHMM and one-dimensional state-switching Ornstein-Uhlenbeck process}

If it is the case that (1) the underlying behavioral state of the continuous-time model must follow a Markov chain rather than a Markov process, and (2) the emission distributions of the CarHMM are gaussian, then the CarHMM and the state-switching continuous model are equivalent. This allows the theoretically grounded continuous-time state-switching model to be used in the computational convieneint HMM (and therefore HHMM) framework. In addition, it gives new intpretation to the learned parameters of the CarHMM in the context of an Ornstein-Uhlenbeck process.

A one-dimensional state-switching Ornstein-Uhlenbeck process $y^*$ is the solution to the following stochastic differential equation:
%
$$dy^*_t = \beta_{x^*_t}(\gamma_{x^*_t} - y^*_t)dt + \omega_{x^*_t} dW_t$$
%
where $x^*_t$ is the fine-scale behavior of the animal at time $t$, $\beta_{x^*_t}$ relates to rate at which the process returns to its mean value, $\gamma_{x^*_t}$ is the long-term mean value of the process, $\omega_{x^*_t}$ is related to short-term variance, and $W$ is a Wiener process. As before, $x^*_t$ is described by an unobserved Markov process. The solution to this equation is known to be the following \cite{Michelot:2019}:
\begin{align*}
y^*_{t+\delta} \sim \mathcal{N}\left((1-e^{-\beta_{x^*_t}\delta})\gamma_{x^*_t} + e^{-\beta_{x^*_t}\delta} y^*_t,\quad \frac{\omega_{x^*_t}^2}{2\beta_{x^*_t}} (1-e^{-2\beta_{x^*_t}\delta})\right)
\end{align*}
Now, suppose that $\delta$ is constant for all observations, as is the case for hidden Markov models. In addition, introduce the following transformations:
\begin{align*}
\mu_{x^*_t} = \gamma_{x^*_t}, \qquad \phi_{x^*_t} = e^{-\beta_{x^*_t}\delta}, \qquad \sigma^2_{x^*_t} = \frac{\omega_{x^*_t}^2}{2\beta_{x^*_t}} (1-e^{-2\beta_{x^*_t}\delta})
\end{align*}
Then, we have the following:
\begin{align*}
y^*_{t+\delta} \sim \mathcal{N}\left((1-\phi_{x^*_t})\mu_{x^*_t} + \phi_{x^*_t} y^*_t,\quad \sigma_{x^*_t}^2 \right)
\end{align*}
%
If $\delta$ is fixed and $x^*_t$ is adjusted to follow a Markov chain rather than a Markov process, then this model is equivalent to the CarHMM with normal emission probabilities. Note that all of the parameter transformations above are one-to-one, so it is easy to go from the CarHMM to the continuous model and back again. This allows for the principled construction of the continuous-time model to be combined with the computational convenience of the CarHMM.

\fi
