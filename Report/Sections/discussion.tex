% !TeX root = ../main.tex

%\section{Discussion}

We presented a collection of HMM models which can be combined together to form increasingly complex hierarchical models to match the complexity of particular problems faced by researchers. This flexible framework can be used to deal with complicated dependence structures within time-series data.

Traditional HMMs can be used to model a state-switching process with conditionally independent observations and Markovian dynamics when conditioned of the hidden state. However, many real-world processes are more complicated than this and require more complex models.

The CarHMM generalizes the HMM by explicitly modeling auto-correlation in the emission distributions of the HMM \citep{Lawler:2019}. CarHMMs also maintain the structure needed to evaluate the likelihood using the forward algorithm. In our Normal model formulation, we have added only one additional parameter, $\phi^{(i)}$, per possible hidden state. Several useful model selection tools such as the lag plot can test if there is significant auto-correlation within an observation sequence.

Although the CarHMM can incorporate auto-correlation into the structure of an HMM, it can break down when observations are taken at irregular time intervals. A common solution to this issue is to use a continuous-time method such as the state-switching OU processes described by Michelot \& Blackwell (2019). Most continuous time models require relatively slow MCMC algorithms to perform inference, and as a result are not easily incorporated into the HHMM structure. However, we show in Theorem 1 that certain continuous-time methods are equivalent to an CarHMM under certain conditions (see appendix for proof).

For simultaneous observed processes taking place at different time scales, the HHMM \citep{Barajas:2017,Adam:2019} utilizes hierarchical structures to jointly model both as HMMs. In particular, each hidden state of the coarse-scale HMM is assumed to emit both an observation $Y_t$ as well as another fine-scale HMM with hidden states $X^*_t$ and observations $Y^*_t$. 

For processes with very high sampling frequencies and/or with intricate fine-scale structure, it is possible to generalize the HHMM such that the fine-scale model can be any model which admits an easy-to-calculate likelihood. For example, if the sampling rate of the fine-scale process is very high, then the fine-scale model can be described by a simple probability distribution over the summary statistics of a moving window of observations. 

Combining these models together should be done with care, as it is important to balance the need to effectively capture the process in question with the need to avoiding over-fitting and slow parameter estimation.

One way to temper model complexity is to reduce the dimension of the parameter space by forcing fine-scale states to be shared across the coarse-scale states. Even still, model complexity inevitably grows rapidly as hierarchical structures are stacked on top of each other.

An example of balancing model complexity with efficient fitting is presented in our simulation study. Sub-dive behavioral states were shared across dive types to reduce model complexity, and by far the fastest model to train ($\approx$ 15 minutes) was the CarHMM-DFT, which had no hierarchical component. Even still, the CarHMM-DFT had near-perfect accuracy when decoding sub-dive behavioral states of the simulated whale.
However, this model is not sufficient if ecologists wish to understand to joint relationship between dive type and intra-dive behaviour. 

The simulation study also shows that the observed Fisher information serves as a suitable approximation for the standard errors of parameter estimates in most cases. One notable exception is that the standard errors of the probability transition matrix estimates ($\hat \Gamma$ and $\hat \Gamma^*$) tend to be overestimated by the observed Fisher information.

Finally, we used the CarHHMM-DFT to model the behavior of a killer whale off the coast of British Columbia, Canada. The CarHHMM-DFT was able to simultaneously distinguish three distinct sub-dive behaviors and two dive types. The DFT component proved useful in determining the sub-dive behaviour of the whale, as the mean of the emission distribution of $Z^{*(2)}$ for each sub-dive state was separated by an order of magnitude. Finally, the estimated auto-correlation parameter for $\mathbf{Z}^{*(1)}$, $\phi^*$, was above 0.5 for every dimension and sub-dive type, providing evidence that the conditionally auto-regressive component of the CarHHMM-DFT resulted in a better fit to the data. The introduction of the parameter $\phi^*$ also allows $\mathbf{Z}^{*(1)}$ to be interpreted as a state-switching OU process (see appendix).

Because traditional information criteria tend to overestimate the number of states in biological processes \citep{Pohle:2017}, the number of dive types and sub-dive behaviors was selected in an ad-hoc manner. There does appear to be some heterogeneity within dive types, and future work can be done to determine the optimal number of dive types and within-dive behaviors.