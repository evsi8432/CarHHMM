% !TeX root = ../main.tex

%\section{Discussion}

We presented a collection of HMM models which can be combined together in increasingly complex hierarchical models to match the complexity of particular problems faced by practitioners. Traditional HMMs can be used to model a state-switching process with conditionally independent observations and Markovian dynamics within the hidden state. However, many real-world processes are more complicated than this.

For observations that do not appear to be conditionally independent in time, the CarHMM is a good choice. The CarHMM explicitly models auto-correlation in the emission distributions of the HMM while maintaining the structure needed to evaluate the likelihood using the forward algorithm. In our normal model formulation, we have added only one additional parameter, $\phi^{(i)}$, per possible hidden state \citep{Lawler:2019}. In addition, we prove in the appendix that under certain circumstances the CarHMM is equivalent to a state-switching Ornstein-Uhlenbeck process. Several useful model selection tools such as the lag plot can test if there is significant auto-correlation within the observation sequence.

For simultaneous observed processes taking place at different time scales, the HHMM \citep{Barajas:2017,Adam:2019} utilizes hierarchical structures to jointly model both as HMMs. A coarse-scale HMM is assumed to emit both an observation $Y_t$ as well as another fine-scale HMM with hidden states $X^*_t$ and observations $Y^*_t$. 

Finally, for processes with very high sampling frequencies and/or with intricate fine-scale structure, the HHMM structure developed by \citep{Barajas:2017,Adam:2019} can be generalized so the fine-scale model can be any model which admits an easy-to-calculate likelihood. For example, if the sampling rate of the fine-scale process is very high, then the fine-scale model can be a simple probability distribution over the summary statistics of a moving window of observations. In addition, researchers can recursively stack hierarchical HMM models together like building blocks to create increasingly complex models. This should be done with care, as it is important to balance the need to effectively capture the process in question with the need to avoiding over-fitting and slow parameter estimation. One way to temper model complexity is to reduce the dimension of the parameter space by forcing fine-scale states to be shared across the coarse-scale states, but model complexity inevitably grows rapidly as hierarchical structures are stack on top of each other.

An example of balancing model complexity with fast model fitting is presented in the simulation study here. While (fine-scale) sub-dive behavioral states were shared across (coarse-scale) dive types to reduce model complexity for the hierarchical models, by far the fastest model to train (~15 minutes) was the simple CarHMM-DFT. This models had no hierarchical component but near-perfect accuracy when decoding sub-dive behavioral states of the simulated whale. This model would be the preferred one if simultaneous modeling of dive types was not required. However, this model is deficient if ecologists wish to understand to joint relationship between dive type and intra-dive behaviour. 

The simulation study also shows that the observed fisher information serves as a suitable approximation for the standard errors of parameter estimates in most cases. One notable exception is that the standard errors of the probability transition matrix estimates, $\hat \Gamma$ and $\hat \Gamma^*$, tend to be overestimated by the observed fisher information.

Finally, we used the CarHHMM-DFT to model the behavior of a killer whale off the coast of British Columbia, Canada. The CarHHMM-DFT was able to distinguish three distinct sub-dive behaviors and two dive types simultaneously due to its hierarchical structure. The DFT component proved useful in determining the sub-dive behaviors of the whale, as the mean of the emission distribution of $Z^{*(2)}$ was separated by an order of magnitude between each sub-dive state. Finally, the learned auto-correlation parameter for $\mathbf{Z}^{*(1)}$, $\phi$, was above 0.5 for every dimension and sub-dive type, providing evidence that the conditionally auto-regressive component of the CarHHMM-DFT resulted in a better fit of the data. The introduction of the parameter $\phi$ allows $\mathbf{Z}^{*(1)}$ to be interpreted as a state-switching OU process (see appendix).

Because traditional information criteria tend to overestimate the number of states in biological processes (\citep{Pohle:2017}), the number of dive types and sub-dive behaviors was selected in an ad-hoc manor. There does appear to be some heterogeneity within dive types, and future work can be done to determine the optimal number of dive types and within-dive behaviors.