% !TeX root = ../main.tex

\section{Results regarding CarHMMs and the Ornstein-Uhlenbeck process}

\subsection{Continuous Time models}

Although the CarHMM can incorporate auto-correlation into the structure of an HMM, they break down when observations are taken at irregular time intervals since the probability transition matrix $\Gamma$ would then depend upon the time step between observations. A common solution to this issue is to use a continuous-time method such as the one introduced by Michelot et al \cite{Michelot:2019}, which models the movement of an animal as an Ornstein-Uhlenbeck process with parameters that depend upon the underlying behavioural state of the animal. Namely, a state-switching Ornstein-Uhlenbeck process $y$ is the solution to the following stochastic differential equation:
%
$$dy_t = \beta_{x_t}(\gamma_{x_t} - y_t)dt + \omega_{x_t} dW_t$$
%
where $x_t$ is the fine-scale behavior of the animal at time $t$, $\beta_{x_t}$ relates to rate at which the process returns to its mean value, $\gamma_{x_t}$ is the long-term mean value of the process, $\omega_{x_t}$ is related to short-term variance, and $W$ is a Wiener process. $x_t$ is assumed to follow an unobserved continuous-time Markov process. Here, $t \in \mathbb{R}$ is a continuous time stamp rather than an index. If the behavioral state $x_t$ is known and does not change between observations, the solution is known to be the following \cite{Michelot:2019}:
%
\begin{align}
y_{t+\Delta t} | x_{t:t+\Delta t} \sim \mathcal{N}\left((1-e^{-\beta_{x_t}\Delta t})\gamma_{x_t} + e^{-\beta_{x_t}\Delta t} y_t,\quad \frac{\omega_{x_t}^2}{2\beta_{x_t}} (1-e^{-2\beta_{x_t}\Delta t})\right)
\label{eqn:OU_sol}
\end{align}
%
where $\Delta t$ is the distance between any two observations $Y_t$ and $Y_{t+\delta t}$. Continuous time models can be built up from arbitrarily complex stochastic differential equations and allow for uneven step lengths in the observations sequence $Y$. However, most continuous time models require MCMC algorithms to perform inference over and as a result are not easily incorporated into the HHMM structure.

\subsection{Equivalence of CarHMM and OU process}

Once the CarHMM is fit by maximizing the likelihood in equation (\ref{CarHMM_likelihood}), the emission distributions can be interpreted as the solution of a state-switching Ornstein-Uhlenbeck process similar to the one introduced by Michelot et al \cite{Michelot:2019}. In order for this to be the case, however, the following two conditions must be met:
\begin{enumerate}
	\item The underlying behavioral state of the continuous-time model follows a Markov chain rather than a Markov process.
	\item The emission distributions of the CarHMM are normal.
\end{enumerate}
If this is the case, then the CarHMM is equivalent to a state-switching Ornstein-Uhlenbeck process. This gives new interpretation to the learned parameters of the CarHMM in the context of a continuous-time model.

\subsubsection{Proof}

Suppose that $\Delta t$ is constant for all observations. In addition, introduce the following transformations:
%
\begin{align}
\mu_{x_t} = \gamma_{x_t}, \qquad \phi_{x_t} = e^{-\beta_{x_t}\Delta t}, \qquad \sigma^2_{x_t} = \frac{\omega_{x_t}^2}{2\beta_{x_t}} (1-e^{-2\beta_{x_t}\Delta t}) \label{eqn:CarHMM_to_OU} \\
\gamma_{x_t} = \mu_{x_t}, \qquad \beta_{x_t} = -\frac{\log(\phi_{x_t})}{\Delta t}, \qquad \omega_{x_t} = \sqrt{\frac{2\sigma^{2}_{x_t}\log(\phi_{x_t})}{\Delta t (\phi_{x_t}^2-1)}}
\label{eqn:OU_to_CarHMM}
\end{align}
%
Combining equation (\ref{eqn:CarHMM_to_OU}) with equation (\ref{eqn:OU_sol}) gives the following:
%
\begin{align*}
y_{t+\Delta t} | x_{t:t+\Delta t} \sim \mathcal{N}\left((1-\phi_{x_t})\mu_{x_t} + \phi_{x_t} y_t,\quad \sigma_{x_t}^2 \right)
\end{align*}
%
If $\Delta t$ is fixed and $x_t$ is adjusted to follow a Markov chain rather than a Markov process, then this model is equivalent to the CarHMM with normal emission probabilities. Note that all of the parameter transformations above are one-to-one, so it is easy to go from the CarHMM to the continuous model and back again. This allows for the principled construction of the continuous-time model to be combined with the computational convenience of the CarHMM.

\subsection{Generalization to unequal time steps}

Using this intuition, we generalize the CarHMM to data with unequal time steps $\Delta t$. First, it is necessary to assume that the behavioral state $x_t$ only changes once between observations, which is a fair assumption if each time step $\Delta t$ is sufficiently small. Next, the following definitions are introduced:
%
$$\Gamma_{\Delta t,\Lambda} = \begin{pmatrix} 
1-e^{-\lambda_1 \Delta t} & p_{12} e^{-\lambda_1 \Delta t} & \dots & p_{1N} e^{-\lambda_1 \Delta t} \\
p_{21} e^{-\lambda_2 \Delta t} & 1-e^{-\lambda_2 \Delta t} & \dots & p_{2N} e^{-\lambda_2 \Delta t} \\
\vdots & \vdots & \ddots & \vdots \\
p_{N1} e^{-\lambda_N \Delta t} & p_{N2} e^{-\lambda_N \Delta t} & \dots  & 1-e^{-\lambda_N \Delta t}
\end{pmatrix}$$
%
where $\Lambda = \{p,\lambda\}$, $p_{ij}$ is the probability that the animal moves from state $i$ to state $j$ given that it left state $i$ such that $\sum_{j \neq i} p_ij = 1$, and $\lambda_i$ is the rate at which the animal leaves behavioral state $i$. Finally, we just note that the parameters in equation (\ref{CarHMM_to_OU}) become functions of the time step between observations $\Delta t$:
%
\begin{align*}
\mu_{x_t}= \gamma_{x_t}, \qquad \phi_{x_t}(\Delta t) = e^{-\beta_{x_t}\Delta t}, \qquad \sigma^2_{x_t}(\Delta t) = \frac{\omega_{x_t}^2}{2\beta_{x_t}} (1-e^{-2\beta_{x_t}\Delta t})
\end{align*}
%
Then the likelihood of the CarHMM with unequal time steps is the following:
%
$$\calL_{\text{CarHMM}}(y;\theta,\Lambda,\delta) = \delta \prod_{t=2}^{T} \Gamma_{\Delta t,\Lambda} P(y_t;\theta) \mathbf{1}$$
%
where:
%
$$P(y_t;\theta) = \text{diag}(p_\theta(y_t|y_{t-1}, \Delta t, X_t = x_1), . . . , p_\theta(y_t|y_{t-1}, \Delta t, X_t = x_N )), \qquad t > 1$$
%
and:
%
$$p_\theta(y_t|y_{t-1}, \Delta t, X_t = x_t) = \mathcal{N}\left((1-\phi_{x_t}(\Delta t))\mu_{x_t} + \phi_{x_t}(\Delta t) y_{t-1},\quad \sigma_{x_t}^2(\Delta t) \right).$$
%
now $\calL_{\text{CarHMM}}(y;\theta,\Lambda,\delta)$ is simply maximized with respect to $\theta = \{\beta,\omega,\gamma\}$, $\Lambda = \{p,\lambda\}$, and $\delta$ to find the maximum likelihood estimate of the CarHMM with uneven time steps.

\iffalse



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\subsection{Transition Parameters}

The probability transition matrix $\bfA$ must be right stochastic, meaning that its entries must be between 0 and 1 and its rows must add to one. Lawler et al do not go into detail about how to parameterize $\bfA$ such that it is both identifiable and constraint free, but the following parameterization does the trick \cite{Barajas:2017}:
%
\begin{align*}
	a_{ij} = \frac{\exp(\eta_{ij})}{\sum_{\ell = 1}^k \exp(\eta_{i\ell})}, \qquad \eta_{ii} = 0 \quad \forall i \in \{1,\ldots,k\}
\end{align*}
%
This parameterization simplifies estimation of $\bfA$, since it eliminates constraints when maximizing the likelihood.

In total, there are $5k$ parameters from the emission distributions and $k^2 - k$ parameters from the probability transition matrix, leaving a total of $k^2 + 4k$ parameters to estimate.

\subsection{Likelihood Evaluation and State Decoding}

While it is possible to use the EM algorithm to estimate these parameters, Lawler et al use direct likelihood maximization instead. In order to do this, it is necessary to evaluate the likelihood of the observations $\bfY$ given parameters $\mu_{RL,b}, \phi_b, \sigma_b^2, c_b, \rho_b$, and $\bfA$. Let the emission probability density of $\theta_t$ and $d_t$ given $d_{t-1}$ and $b_t$ be denoted as $f(y_t|b_t,y_{t-1})$. Additionally, recall that the stationary distribution of $\bfA$ is $\delta$ such that $\delta \bfA = \delta$ and $\sum_i \delta_i = 1$. Assuming that the initial behavior of the animal is drawn from this stationary distribution, the likelihood of $\bfY$ can be calculated sequentially using the following recurrence relation:
%
\begin{align*}
	\alpha_1(b_1) = p(y_1,b_1) &= f(y_1|b_1)\delta_{b_1} \\\\\\
	%
	\alpha_t(b_t) = p(y_{1:t},b_t) &= f(y_t|b_t,y_{t-1})\sum_{b_{t-1} \in \{1,\ldots,k\}}a_{b_{t-1},b_t}p(y_{1:t-1},b_{t-1}) \\
	%
	&= f(y_t|b_t,y_{t-1})\sum_{b_{t-1} \in \{1,\ldots,k\}}a_{b_{t-1},b_t} \alpha_{t-1}(b_{t-1}) \\\\\\
	%
	L(\bfA,\mu_{RL,b},\phi_b,\sigma_b^2,c_b,\rho_b; \bfY) = p(y_{1:T-1}) &= \sum_{b_{T-1} \in \{1,\ldots,k\}} \alpha_{T-1}(b_{T-1}) 
\end{align*}	
%
Note that $\alpha_t$ is a $k$-dimensional vector, where each dimension corresponds to a distinct behavioral state. The process of evaluating this recurrence relationship sequentially is called the \textbf{forward algorithm} and is vital to estimating the parameters of HMMs via likelihood maximization. One particularly convenient feature of the forward algorithm is that its time complexity is linear in both the number of time steps and behavioral states. The forward algorithm can also be expressed more concisely as a matrix product:
%
$$L(\bfA,\mu_{RL,b},\phi_b,\sigma_b^2,c_b,\rho_b; \bfY) = \delta \bfP(y_1) \prod_{i=1}^{T-1} \bfA \bfP(y_t,y_{t-1}) \mathbf{1}$$
$$\bfP(y_1) = \text{Diag}\left( f(y_1|B_1 = 1), \ldots, f(y_1|B_1 = k) \right)$$
$$\bfP(y_t,y_{t-1}) = \text{Diag}\left( f(y_t|B_1 = 1, y_{t-1}), \ldots, f(y_t|B_1 = k, y_{t-1}) \right)$$
%
$\mathbf{1}$ is a column vector of ones. Once the emission and transition parameters have been estimated, the most likely sequence of underlying behavioral states $\bfB$ can be found using the Viterbi algorithm \cite{Viterbi:1967}.

\subsection{Similarity to Previous Work}

Lawler et al are not the first to incorporate auto correlation within a hidden Markov model in an ecological setting. Woriskey et al \cite{Whoriskey:2016} developed a hidden Markov model called the HMMM, which is formulated as the following:
%
$$\left(\bfY_t - \bfY_{t-1} |\bfY_{t-1} - \bfY_{t-2}, b_{t-1}\right) \sim \mathcal{N}\left( \phi_{b_{t-1}} T(\theta_{b_{t-1}})(\bfY_{t-1} - \bfY_{t-2}),\Sigma\right)$$
%
where $\phi_b$ is the state-dependent auto-correlation term, $\theta_b$ is the state-dependent mean turning angle, $T(\theta)$ is a standard rotation matrix, and $\Sigma$ is a state-independent two-dimensional covariance matrix. Like Lawler, Woriskey fits $\phi_b$, $\theta_b$, and $\Sigma$ using likelihood maximization. The details of the two approaches are different, but both are similar in that they use auto-correlation within the emission probabilities of an HMM to model animal movement in two dimensions.

In addition, many continuous-time approaches model auto-correlation to account for non-uniform time intervals between time steps. For example, Michelot et al. \cite{Michelot:2019} model $\bfY$ as an integrated Ornstein-Uhlenbeck process, where the position $\bfY_t$ and velocity $\bfV_t$ are modeled as draws from a normal distribution which depends upon $\bfV_{t-1}$, $\bfY_{t-1}$, the behavioral state $b_{t-1}$, and the time step between observations $\Delta_{t-1}$. It is straightforward to adapt this continuous-time approach to an HMM by setting $\Delta_{t}$ to be constant for all values of $t$ and assuming that the behavior of the animal does not change between time steps. In fact, if the CarHMM is altered so that the emission distribution of $d_t$ is normal, then the resulting sequence of step-lengths is equivalent to a one-dimensional state-switching Ornstein-Uhlenbeck process (see the appendix for details).

While the idea of auto-correlation within HMMs is not new, Lawler et al are novel in their specific implementation of an auto-correlated HMM. In addition, they detail several additional tools for data preprocessing and interpretation which are useful when applying HMMs in an ecological setting.

\fi